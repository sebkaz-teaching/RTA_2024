{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przetwarzanie Spark DataFrame\n",
    "\n",
    "## Przygotowanie danych\n",
    "\n",
    "```bash\n",
    "mkdir data\n",
    "cd data\n",
    "curl -L -o donation.zip http://bit.ly/1Aoywaq\n",
    "unzip donation.zip\n",
    "unzip 'block_*.zip'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe \n",
    "prev = spark.read.csv(\"data/block*.csv\")\n",
    "prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev.show(2)\n",
    "\n",
    "prev.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dodatkowe opcje z header i wartości null \n",
    "parsed = spark.read.option(\"header\", \"true\")\\\n",
    ".option(\"nullValue\", \"?\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".csv(\"data/block*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.show(5)\n",
    "\n",
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### obsługiwane formaty \n",
    "\n",
    "- parquet\n",
    "- orc\n",
    "- json\n",
    "- jdbc\n",
    "- avro\n",
    "- yrxy\n",
    "- image\n",
    "- libsvm\n",
    "- binary\n",
    "- xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.write.format(\"parquet\").save(\"data/block2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = spark.read.format(\"parquet\").load(\"data/block2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schematy danych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"Date\", StringType(), True),\n",
    "  StructField(\"Open\", DoubleType(), True),\n",
    "  StructField(\"High\", DoubleType(), True),\n",
    "  StructField(\"Low\", DoubleType(), True),\n",
    "  StructField(\"Close\", DoubleType(), True),\n",
    "  StructField(\"Volume\", IntegerType(), True),\n",
    "  StructField(\"Name\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "ddlSchemaStr = \"\"\"Date STRING, Open FLOAT, High FLOAT, \n",
    "Low FLOAT, Close FLOAT, Voulme INT, Name String \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"header\", True)\\\n",
    ".csv(\"AAPL_2006-01-01_to_2018-01-01.csv\", schema=ddlSchemaStr)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dane niustrukturyzowane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file test.json\n",
    "\n",
    "{\n",
    " \"id\": \"0001\",\n",
    " \"type\": \"donut\",\n",
    " \"name\": \"Cake\",\n",
    " \"ppu\": 0.55,\n",
    " \"batters\":\n",
    "  {\n",
    "   \"batter\":\n",
    "    [\n",
    "     { \"id\": \"1001\", \"type\": \"Regular\" },\n",
    "     { \"id\": \"1002\", \"type\": \"Chocolate\" },\n",
    "     { \"id\": \"1003\", \"type\": \"Blueberry\" }\n",
    "    ]\n",
    "  },\n",
    " \"topping\":\n",
    "  [\n",
    "   { \"id\": \"5001\", \"type\": \"None\" },\n",
    "   { \"id\": \"5002\", \"type\": \"Glazed\" },\n",
    "   { \"id\": \"5005\", \"type\": \"Sugar\" },\n",
    "   { \"id\": \"5007\", \"type\": \"Powdered Sugar\" },\n",
    "   { \"id\": \"5006\", \"type\": \"Chocolate with Sprinkles\" },\n",
    "   { \"id\": \"5003\", \"type\": \"Chocolate\" },\n",
    "   { \"id\": \"5004\", \"type\": \"Maple\" }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDFjson = spark.read.json(\"test.json\", multiLine = \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDFjson.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF = rawDFjson.withColumnRenamed(\"id\", \"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batDF = sampleDF.select(\"key\", \"batters.batter\")\n",
    "batDF.printSchema()\n",
    "batDF.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "bat2DF = batDF.select(\"key\", explode(\"batter\").alias(\"new_batter\"))\n",
    "bat2DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat2DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat2DF.select(\"key\", \"new_batter.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalBatDF = (sampleDF\n",
    "        .select(\"key\",  \n",
    "explode(\"batters.batter\").alias(\"new_batter\"))\n",
    "        .select(\"key\", \"new_batter.*\")\n",
    "        .withColumnRenamed(\"id\", \"bat_id\")\n",
    "        .withColumnRenamed(\"type\", \"bat_type\"))\n",
    "finalBatDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topDF = (sampleDF\n",
    "        .select(\"key\", explode(\"topping\").alias(\"new_topping\"))\n",
    "        .select(\"key\", \"new_topping.*\")\n",
    "        .withColumnRenamed(\"id\", \"top_id\")\n",
    "        .withColumnRenamed(\"type\", \"top_type\")\n",
    "        )\n",
    "topDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksploracyjna Analiza Danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zweryfikuj schemat danych\n",
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdz wartosci dla pierwszego rzedu\n",
    "parsed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ile przypadkow \n",
    "parsed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target \"is_match\" liczba zgodnych i niezgodnych rekordow\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inne agregaty agg\n",
    "from pyspark.sql.functions import avg, stddev, stddev_pop\n",
    "\n",
    "parsed.agg(avg(\"cmp_sex\"), stddev(\"cmp_sex\"), stddev_pop(\"cmp_sex\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polecenia sql - przypisanie nazwy dla silnika sql - tabela przejsciowa\n",
    "parsed.createOrReplaceTempView(\"dane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" SELECT is_match, COUNT(*) cnt FROM dane group by is_match order by cnt DESC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zbiorcze statystyki \n",
    "summary = parsed.describe()\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> która zmienna lepiej opisze dane c1 czy c2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statystyki dla poszczegolnych klas\n",
    "\n",
    "# filtrowanie sql\n",
    "matches = parsed.where(\"is_match = true\")\n",
    "\n",
    "\n",
    "# filtrowanie pyspark\n",
    "misses = parsed.filter(col(\"is_match\") == False)\n",
    "\n",
    "match_summary = matches.describe()\n",
    "miss_summary = misses.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabele przestawne spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p = summary.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p = summary_p.set_index('summary').transpose().reset_index()\n",
    "summary_p = summary_p.rename(columns={'index':'field'})\n",
    "summary_p = summary_p.rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryT = spark.createDataFrame(summary_p)\n",
    "summaryT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryT.printSchema() # czy dobre typy danych ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "for c in summaryT.columns:\n",
    "    if c == 'field':\n",
    "        continue\n",
    "    summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryT.printSchema() # teraz lepiej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_summary(desc):\n",
    "    desc_p = desc.toPandas()\n",
    "    desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
    "    desc_p = desc_p.rename(columns={'index':'field'})\n",
    "    desc_p = desc_p.rename_axis(None, axis=1)\n",
    "    descT = spark.createDataFrame(desc_p)\n",
    "    for c in descT.columns:\n",
    "        if c == 'field':\n",
    "            continue\n",
    "        else:\n",
    "            descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "    return descT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summaryT = pivot_summary(match_summary)\n",
    "miss_summaryT = pivot_summary(miss_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summaryT.createOrReplaceTempView(\"match_s\")\n",
    "miss_summaryT.createOrReplaceTempView(\"miss_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "Select a.field, a.count + b.count total, a.mean - b.mean delta\n",
    "from match_s a inner join miss_s b on a.field = b.field \n",
    "where a.field not in (\"id_1\", \"id_2\")\n",
    "order by delta DESC, total DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> do modelu : `cmp_plz`, `cmp_by`, `cmp_bd`, `cmp_lname_c1`, `cmp_bm`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## score = suma zmiennych\n",
    "zmienne = ['cmp_plz','cmp_by','cmp_bd','cmp_lname_c1','cmp_bm']\n",
    "suma = \" + \".join(zmienne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = parsed.fillna(0, subset=zmienne)\\\n",
    ".withColumn('score', expr(suma))\\\n",
    ".select('score','is_match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocena wartosci progowej\n",
    "def crossTabs(scored, t):\n",
    "    return scored.selectExpr(f\"score >= {t} as above\", \"is_match\")\\\n",
    "    .groupBy(\"above\").pivot(\"is_match\",(\"true\",\"false\"))\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossTabs(scored, 4.0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossTabs(scored, 2.0).show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
