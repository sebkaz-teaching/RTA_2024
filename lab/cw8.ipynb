{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych\n",
    "\n",
    "```bash\n",
    "mkdir data\n",
    "cd data\n",
    "curl -L -o donation.zip http://bit.ly/1Aoywaq\n",
    "unzip donation.zip\n",
    "unzip 'block_*.zip'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe \n",
    "prev = spark.read.csv(\"data/block*.csv\")\n",
    "prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev.show(2)\n",
    "\n",
    "prev.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dodatkowe opcje z header i warto≈õci null \n",
    "parsed = spark.read.option(\"header\", \"true\")\\\n",
    ".option(\"nullValue\", \"?\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".csv(\"data/block*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.show(5)\n",
    "\n",
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inne formaty \n",
    "\n",
    "- parquet\n",
    "- orc\n",
    "- json\n",
    "- jdbc\n",
    "- avro\n",
    "- yrxy\n",
    "- image\n",
    "- libsvm\n",
    "- binary\n",
    "- xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.write.format(\"parquet\").save(\"data/block2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = spark.read.format(\"parquet\").load(\"data/block2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schematy danych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"Date\", StringType(), True),\n",
    "  StructField(\"Open\", DoubleType(), True),\n",
    "  StructField(\"High\", DoubleType(), True),\n",
    "  StructField(\"Low\", DoubleType(), True),\n",
    "  StructField(\"Close\", DoubleType(), True),\n",
    "  StructField(\"Volume\", IntegerType(), True),\n",
    "  StructField(\"Name\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "ddlSchemaStr = \"\"\"Date STRING, Open FLOAT, High FLOAT, \n",
    "Low FLOAT, Close FLOAT, Voulme INT, Name String \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"header\", True)\\\n",
    ".csv(\"AAPL_2006-01-01_to_2018-01-01.csv\", schema=ddlSchemaStr)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dane niustrukturyzowane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%file test.json\n",
    "\n",
    "{\n",
    " \"id\": \"0001\",\n",
    " \"type\": \"donut\",\n",
    " \"name\": \"Cake\",\n",
    " \"ppu\": 0.55,\n",
    " \"batters\":\n",
    "  {\n",
    "   \"batter\":\n",
    "    [\n",
    "     { \"id\": \"1001\", \"type\": \"Regular\" },\n",
    "     { \"id\": \"1002\", \"type\": \"Chocolate\" },\n",
    "     { \"id\": \"1003\", \"type\": \"Blueberry\" }\n",
    "    ]\n",
    "  },\n",
    " \"topping\":\n",
    "  [\n",
    "   { \"id\": \"5001\", \"type\": \"None\" },\n",
    "   { \"id\": \"5002\", \"type\": \"Glazed\" },\n",
    "   { \"id\": \"5005\", \"type\": \"Sugar\" },\n",
    "   { \"id\": \"5007\", \"type\": \"Powdered Sugar\" },\n",
    "   { \"id\": \"5006\", \"type\": \"Chocolate with Sprinkles\" },\n",
    "   { \"id\": \"5003\", \"type\": \"Chocolate\" },\n",
    "   { \"id\": \"5004\", \"type\": \"Maple\" }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDFjson = spark.read.json(\"test.json\", multiLine = \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDFjson.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF = rawDFjson.withColumnRenamed(\"id\", \"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batDF = sampleDF.select(\"key\", \"batters.batter\")\n",
    "batDF.printSchema()\n",
    "batDF.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "bat2DF = batDF.select(\"key\", explode(\"batter\").alias(\"new_batter\"))\n",
    "bat2DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat2DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat2DF.select(\"key\", \"new_batter.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalBatDF = (sampleDF\n",
    "        .select(\"key\",  \n",
    "explode(\"batters.batter\").alias(\"new_batter\"))\n",
    "        .select(\"key\", \"new_batter.*\")\n",
    "        .withColumnRenamed(\"id\", \"bat_id\")\n",
    "        .withColumnRenamed(\"type\", \"bat_type\"))\n",
    "finalBatDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topDF = (sampleDF\n",
    "        .select(\"key\", explode(\"topping\").alias(\"new_topping\"))\n",
    "        .select(\"key\", \"new_topping.*\")\n",
    "        .withColumnRenamed(\"id\", \"top_id\")\n",
    "        .withColumnRenamed(\"type\", \"top_type\")\n",
    "        )\n",
    "topDF.show(10, False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
