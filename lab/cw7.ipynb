{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc83df9-c3af-4fe2-a414-891e9c3f72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file model.py\n",
    "\n",
    "from joblib import dump\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    " \n",
    "ran_gen = np.random.RandomState(44)\n",
    " \n",
    "X = 0.4 * ran_gen.randn(500,2)\n",
    "X = np.round(X, 3)\n",
    "X_train = np.r_[X+2, X-2]\n",
    " \n",
    "clf = IsolationForest(n_estimators=50, max_samples=500, random_state=ran_gen, contamination=0.01)\n",
    "clf.fit(X_train)\n",
    " \n",
    "dump(clf, './isolation_forest.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d692387-a231-4e49-ba4d-68a5f743aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225339ce-64ee-41bc-8346-6f4638aaedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file transaction_producer.py\n",
    " \n",
    "import json\n",
    "import time \n",
    "import logging\n",
    "import socket\n",
    "from datetime import datetime\n",
    "from numpy.random import uniform, choice, randn\n",
    " \n",
    "from random import random as r\n",
    " \n",
    "import numpy as np\n",
    "from confluent_kafka import Producer\n",
    " \n",
    " \n",
    "KAFKA_BROKER = 'broker:9092'\n",
    "TRANSACTION_TOPIC = 'transactions'\n",
    "LAG = 0.5\n",
    "PROBABILITY_OUTLIER = 0.05\n",
    " \n",
    "def create_producer():\n",
    "    try:\n",
    "        producer = Producer({\n",
    "        \"bootstrap.servers\":KAFKA_BROKER,\n",
    "        \"client.id\": socket.gethostname(),\n",
    "        \"enable.idempotence\": True,\n",
    "        \"batch.size\": 64000,\n",
    "        \"linger.ms\":10,\n",
    "        \"acks\": \"all\",\n",
    "        \"retries\": 5,\n",
    "        \"delivery.timeout.ms\":1000\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logging.exception(\"nie mogę utworzyć producenta\")\n",
    "        producer = None\n",
    "    return producer\n",
    " \n",
    " \n",
    "_id = 0 \n",
    "producer = create_producer()\n",
    " \n",
    "if producer is not None:\n",
    "    while True:\n",
    "        if r() <= PROBABILITY_OUTLIER:\n",
    "            X_test = uniform(low=-4, high=4, size=(1,2))\n",
    "        else:\n",
    "            X = 0.3 * randn(1,2)\n",
    "            X_test = (X + choice(a=[2,-2], size=1, p=[0.5, 0.5]))\n",
    "        X_test = np.round(X_test, 3).tolist()\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        record = {\n",
    "        \"id\": _id,\n",
    "        \"data\": X_test,\n",
    "        \"current_time\" : current_time\n",
    "        }\n",
    "        record = json.dumps(record).encode(\"utf-8\")\n",
    "        producer.produce(topic= TRANSACTION_TOPIC, value=record)\n",
    "        producer.flush()\n",
    "        _id +=1 \n",
    "        time.sleep(LAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a95a4-774f-48ae-89ee-6dff8da91ea2",
   "metadata": {},
   "source": [
    "# generowanie strumienia danych \n",
    "\n",
    "Uruchom terminal i odpal kod producenta.\n",
    "```bash\n",
    "python transaction_producer.py\n",
    "```\n",
    "\n",
    "### Weryfikacja Kafki \n",
    "\n",
    "Uruchom nowy terminal i zweryfikuj czy topic `transactions` jest w Kafce\n",
    "```bash\n",
    "cd ~\n",
    "kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n",
    "```\n",
    "\n",
    "Następnie uruchom \n",
    "```bash\n",
    "cd ~\n",
    "kafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic transactions\n",
    "```\n",
    "i sprawdź czy transakcje przechodzą. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8189694-92de-48fa-9e5c-e6529be432ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file outliers_detection.py\n",
    " \n",
    "import json\n",
    "import os\n",
    "import time \n",
    "import numpy as np\n",
    "import socket\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from joblib import load\n",
    "from confluent_kafka import Producer, Consumer\n",
    "from multiprocessing import Process\n",
    " \n",
    "KAFKA_BROKER = 'broker:9092'\n",
    "TRANSACTION_TOPIC = 'transactions'\n",
    "TRANSACTOPM_CG = 'transactions'\n",
    "ANOMALY_TOPIC = 'anomaly'\n",
    "NUM_PARTITIONS = 3\n",
    " \n",
    "MODEL_PATH = os.path.abspath('isolation_forest.joblib')\n",
    " \n",
    "def create_producer():\n",
    "    try:\n",
    "        producer = Producer({\n",
    "        \"bootstrap.servers\":KAFKA_BROKER,\n",
    "        \"client.id\": socket.gethostname(),\n",
    "        \"enable.idempotence\": True,\n",
    "        \"batch.size\": 64000,\n",
    "        \"linger.ms\":10,\n",
    "        \"acks\": \"all\",\n",
    "        \"retries\": 5,\n",
    "        \"delivery.timeout.ms\":1000\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logging.exception(\"nie mogę utworzyć producenta\")\n",
    "        producer = None\n",
    "    return producer\n",
    " \n",
    "def create_consumer(topic, group_id):\n",
    "    try:\n",
    " \n",
    "        consumer = Consumer({\n",
    "          \"bootstrap.servers\": KAFKA_BROKER,\n",
    "          \"group.id\": group_id,\n",
    "          \"client.id\": socket.gethostname(),\n",
    "          \"isolation.level\":\"read_committed\",\n",
    "          \"default.topic.config\":{\n",
    "                    \"auto.offset.reset\":\"latest\",\n",
    "                    \"enable.auto.commit\": False\n",
    "            }\n",
    "        })\n",
    "        consumer.subscribe([topic])\n",
    "    except Exception as e:\n",
    "        logging.exception(\"nie mogę utworzyć konsumenta\")\n",
    "        consumer = None\n",
    "    return consumer\n",
    "def detekcja_anomalii():\n",
    "    consumer = create_consumer(topic=TRANSACTION_TOPIC, group_id=TRANSACTOPM_CG)\n",
    "    producer = create_producer()\n",
    "    clf = load(MODEL_PATH)\n",
    "    while True:\n",
    "        message = consumer.poll()\n",
    "        if message is None:\n",
    "            continue\n",
    "        if message.error():\n",
    "            logging.error(f\"CONSUMER error: {message.error()}\")\n",
    "            continue\n",
    "        record = json.loads(message.value().decode('utf-8'))\n",
    "        data = record['data']\n",
    "        prediction = clf.predict(data)\n",
    "        if prediction[0] == -1 :\n",
    "            score = clf.score_samples(data)\n",
    "            record[\"score\"] = np.round(score, 3).tolist()\n",
    "            _id = str(record[\"id\"])\n",
    "            record = json.dumps(record).encode(\"utf-8\")\n",
    "            producer.produce(topic=ANOMALY_TOPIC, value=record)\n",
    "            producer.flush()\n",
    "    consumer.close()\n",
    "\n",
    " \n",
    "for _ in range(NUM_PARTITIONS):\n",
    "    p = Process(target=detekcja_anomalii)\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a80e3-ad70-4366-8dde-c9c9bc13856b",
   "metadata": {},
   "source": [
    "## Dodaj topic anomaly i uruchom podgląd \n",
    "\n",
    "```bash\n",
    "kafka/bin/kafka-topics.sh --create --topic anomaly --bootstrap-server broker:9092\n",
    "\n",
    "```\n",
    "```bash\n",
    "cd ~\n",
    "kafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic anomaly\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d9461-0434-4216-97e6-1e0740a8d7a2",
   "metadata": {},
   "source": [
    "# Odczytanie strumienia w Apache Spark - Socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5deb90-4bd4-477b-b02b-723b78ceb294",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file socket_stream_start.py\n",
    "\n",
    "from socket import *\n",
    "import time\n",
    "\n",
    "rdd = list()\n",
    "with open(\"MobyDick_full.txt\", 'r') as ad:\n",
    "    for line in ad:\n",
    "        rdd.append(line)\n",
    "\n",
    "HOST = 'localhost'\n",
    "PORT = 9999\n",
    "ADDR = (HOST, PORT)\n",
    "tcpSock = socket(AF_INET, SOCK_STREAM)\n",
    "tcpSock.bind(ADDR)\n",
    "tcpSock.listen(5)\n",
    "\n",
    "\n",
    "while True:\n",
    "    c, addr = tcpSock.accept()\n",
    "    print('got connection')\n",
    "    for line in rdd:\n",
    "        try:\n",
    "            c.send(line.encode())\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            break\n",
    "    c.close()\n",
    "    print('disconnected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92b1fb-23d4-4543-bf05-8550364ff73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file streamWordCount.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"Stream_DF\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    lines = (spark\n",
    "         .readStream\n",
    "         .format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", 9999)\n",
    "         .load())\n",
    "\n",
    "    words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "    word_counts = words.groupBy(\"word\").count()\n",
    "\n",
    "    streamingQuery = (word_counts\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"complete\")\n",
    "         .trigger(processingTime=\"5 second\")\n",
    "         .start())\n",
    "\n",
    "    streamingQuery.awaitTermination()\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1aa6a-a015-46df-88cb-1fd5e9ed9d3a",
   "metadata": {},
   "source": [
    "# Odczytanie strumienia w Apache Spark + Kafka\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ab39b-23b1-4aed-ad28-6d526887d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file raw_app.py\n",
    "\n",
    "## LOAD SPARK SESSION object\n",
    "\n",
    "SERVER = \"broker:9092\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        \n",
    "    raw = (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"broker:9092\")\n",
    "        .option(\"subscribe\", \"transactions\")\n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    query =  (\n",
    "        raw.writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"truncate\", \"false\")\n",
    "        .format(\"console\")\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    query.awaitTermination()\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12eeb1-6c8f-4959-b7a6-e1460b7ab3fd",
   "metadata": {},
   "source": [
    "```bash\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 raw_app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a8fdf-8bdb-4ed1-93fd-8e808c165013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b2400-d4f0-435a-8fe0-80bea67e8583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
