{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1020207c-b542-468a-a885-00d6bbdb7270",
   "metadata": {},
   "source": [
    "# Apache Spark intro\n",
    "\n",
    "\n",
    "Apache Spark is a general-purpose, in-memory computing engine. Spark can be used with Hadoop, Yarn and other Big Data components to harness the power of Spark and improve the performance of your applications. It provides high-level APIs in Scala, Java, Python, R, and SQL.\n",
    "\n",
    "**Spark Architecture**\n",
    "\n",
    "Apache Spark works in a master-slave architecture where the master is called “Driver” and slaves are called “Workers”. The starting point of your Spark application is `sc`, a Spark Context Class instance. It runs inside the driver.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1192/0*XzNeTtwEgIy5yWR_)\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1266/0*-PltnPR9row8iUDo)\n",
    "\n",
    "**Apache Spark**: \n",
    "Sometimes also called Spark Core. The Spark Core implementation is a RDD (Resilient Distributed Dataset) which is a collection of distributed data across different nodes of the cluster that are processed in parallel.\n",
    "\n",
    "\n",
    "**Spark SQL**: \n",
    "The implementation here is DataFrame, which is a relational representation of the data. It provides functions with SQL like capabilities. Also, we can write SQL like queries for our data analysis.\n",
    "\n",
    "\n",
    "**Spark Streaming**: \n",
    "The implementation provided by this library is D-stream, also called Discretized Stream. This library provides capabilities to process/transform data in near real-time.\n",
    "\n",
    "\n",
    "**MLlib**: \n",
    "This is a Machine Learning library with commonly used algorithms including collaborative filtering, classification, clustering, and regression.\n",
    "\n",
    "**GraphX**: \n",
    "This library helps us to process Graphs, solving various problems (like Page Rank, Connected Components, etc) using Graph Theory.\n",
    "\n",
    "\n",
    "Let’s dig a little deeper into Apache Spark (Spark Core), starting with RDD.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb327a64-0391-448e-9867-57c101e02675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b65c12-4ace-42b6-8ff4-cc1d4c8d4ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53bc041-7995-4972-b51c-6a6fee3386a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c170258-541d-4dc0-ba69-73432a3112b5",
   "metadata": {},
   "source": [
    "### RDD\n",
    "\n",
    "- Resilient Distributed Dataset\n",
    "- Podstawowa abstrakcja oraz rdzeń Sparka\n",
    "- Obsługiwane przez dwa rodzaje operacji:\n",
    "    - Akcje:\n",
    "        - operacje uruchamiająceegzekucję transformacji na RDD\n",
    "        - przyjmują RDD jako input i zwracają wynik NIE będący RDD\n",
    "    - Transformacje:\n",
    "        - leniwe operacje\n",
    "        - przyjmują RDD i zwracają RDD\n",
    "\n",
    "- In-Memory - dane RDD przechowywane w pamięci\n",
    "- Immutable \n",
    "- Lazy evaluated\n",
    "- Parallel - przetwarzane równolegle\n",
    "- Partitioned - rozproszone \n",
    "\n",
    "## WAŻNE informacje !\n",
    "\n",
    "Ważne do zrozumienia działania SPARKA:\n",
    "\n",
    "Term                   |Definition\n",
    "----                   |-------\n",
    "RDD                    |Resilient Distributed Dataset\n",
    "Transformation         |Spark operation that produces an RDD\n",
    "Action                 |Spark operation that produces a local object\n",
    "Spark Job              |Sequence of transformations on data with a final action\n",
    "\n",
    "\n",
    "Dwie podstawowe metody tworzenia RDD:\n",
    "\n",
    "Method                      |Result\n",
    "----------                               |-------\n",
    "`sc.parallelize(array)`                  |Create RDD of elements of array (or list)\n",
    "`sc.textFile(path/to/file)`                      |Create RDD of lines from file\n",
    "\n",
    "Podstawowe transformacje\n",
    "\n",
    "Transformation Example                          |Result\n",
    "----------                               |-------\n",
    "`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\n",
    "`map(lambda x: x * 2)`                   |Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())`               |Split each string into words\n",
    "`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\n",
    "`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\n",
    "`union(rdd)`                             |Append `rdd` to existing RDD\n",
    "`distinct()`                             |Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order\n",
    "\n",
    "Podstawowe akcje \n",
    "\n",
    "Action                             |Result\n",
    "----------                             |-------\n",
    "`collect()`                            |Convert RDD to in-memory list \n",
    "`take(3)`                              |First 3 elements of RDD \n",
    "`top(3)`                               |Top 3 elements of RDD\n",
    "`takeSample(withReplacement=True,3)`   |Create sample of 3 elements with replacement\n",
    "`sum()`                                |Find element sum (assumes numeric elements)\n",
    "`mean()`                               |Find element mean (assumes numeric elements)\n",
    "`stdev()`                              |Find element deviation (assumes numeric elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6289576-3208-4807-b05d-00167fc53f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keywords = ['Books', 'DVD', 'CD', 'PenDrive']\n",
    "key_rdd = sc.parallelize(keywords)\n",
    "key_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6d7d3-5aa2-4080-a3a2-f25e10147c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_small = key_rdd.map(lambda x: x.lower()) # transformacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc9d27-90e0-4648-9aef-2e406db50811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_small.collect() # akcja "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8a4f2-f1a7-4b17-8c54-6ebdeabf7a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e567a0-e7df-42c3-af84-b69775435b1e",
   "metadata": {},
   "source": [
    "**Spark’s core data structure**\n",
    "\n",
    "✅: A low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster.\n",
    "\n",
    "❌: However, RDDs are hard to work with directly, so we’ll be using the Spark DataFrame abstraction built on top of RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2226e6-dceb-4e33-83a2-f586f296d69b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MAP REDUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4c64b-1ed1-4080-9a8a-8e262776b63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"new\").getOrCreate()\n",
    "\n",
    "# otrzymanie obiektu SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b73932-1fd1-4f79-8d07-1c16f8523450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Word Count on RDD \n",
    "sc.textFile(\"MobyDick.txt\")\\\n",
    ".map(lambda x: re.findall(r\"[a-z']+\", x.lower())) \\\n",
    ".flatMap(lambda x: [(y, 1) for y in x]).reduceByKey(lambda x,y: x + y)\\\n",
    ".take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b1bf7-8f2d-4b8b-84ab-2759919fd028",
   "metadata": {},
   "source": [
    "## SPARK STREAMING\n",
    "\n",
    "Część Sparka odpowiedzialna za przetwarzanie danych w czasie rzeczywistym. \n",
    "\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-arch.png\"/>\n",
    "\n",
    "Dane mogą pochodzić z różnych źródeł np. sokety TCP, Kafka, etc. \n",
    "Korzystając z poznanych już metod `map, reduce, join, oraz window` można w łatwy sposób generować przetwarzanie strumienia tak jaby był to nieskończony ciąg RDD. \n",
    "Ponadto nie ma problemu aby wywołać na strumieniu operacje ML czy wykresy. \n",
    "\n",
    "Cała procedura przedstawia się następująco: \n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-flow.png\"/>\n",
    "\n",
    "SPARK STREAMING w tej wersji wprowadza abstrakcje zwaną `discretized stream` *DStream* (reprezentuje sekwencję RDD).\n",
    "\n",
    "Operacje na DStream można wykonywać w API JAVA, SCALA, Python, R (nie wszystkie możliwości są dostępne dla Pythona). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447df48-88db-4585-b6ac-3ddc1ee80d51",
   "metadata": {},
   "source": [
    "Spark Streaming potrzebuje minium 2 rdzenie.\n",
    "\n",
    "----\n",
    "- **StreamingContext(sparkContext, batchDuration)** - reprezentuje połączenie z klastrem i służy do tworzenia DStreamów, `batchDuration` wskazuje na granularność batch'y (w sekundach)\n",
    "- **socketTextStream(hostname, port)** - tworzy DStream na podstawie danych napływających ze wskazanego źródła TCP\n",
    "- **flatMap(f), map(f), reduceByKey(f)** - działają analogicznie jak w przypadku RDD z tym że tworzą nowe DStream'y\n",
    "- **pprint(n)** - printuje pierwsze `n` (domyślnie 10) elementów z każdego RDD wygenerowanego w DStream'ie\n",
    "- **StreamingContext.start()** - rozpoczyna działania na strumieniach\n",
    "- **StreamingContext.awaitTermination(timeout)** - oczekuje na zakończenie działań na strumieniach\n",
    "- **StreamingContext.stop(stopSparkContext, stopGraceFully)** - kończy działania na strumieniach\n",
    "\n",
    "Obiekt StreamingContext można wygenerować za pomocą obiektu SparkContext.\n",
    "\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream.png\"/>\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef08b5b-573d-4c5e-bcc0-46a3eb879aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread\n",
    "# and batch interval of 1 second\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount2\")\n",
    "ssc = StreamingContext(sc, 2)\n",
    "\n",
    "# DStream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9998)\n",
    "\n",
    "# podziel każdą linię na wyrazy\n",
    "# DStream jest mapowany na kolejny DStream\n",
    "# words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "\n",
    "# zliczmy każdy wyraz w każdym batchu\n",
    "# DStream jest mapowany na kolejny DStream\n",
    "# pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# DStream jest mapowany na kolejny DStream                  \n",
    "# wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "wordCounts = words.map(lambda word: (word,1)).reduceByKey(lambda x,y: x+y)\n",
    "# wydrukuj pierwszy element\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cefe88-f6ea-4f1a-9fa8-4f4ba7490318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# before start run a stream data\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()\n",
    "ssc.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338377f3-306b-4d98-b96e-8e2b405038cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w konsoli linuxowej netcat Nmap for windows\n",
    "!nc -lk 9998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3cf52-6309-4d19-b8b5-edb13eda3847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%file start_stream.py\n",
    "\n",
    "from socket import *\n",
    "import time\n",
    "\n",
    "rdd = list()\n",
    "with open(\"MobyDick_full.txt\", 'r') as ad:\n",
    "    for line in ad:\n",
    "        rdd.append(line)\n",
    "\n",
    "HOST = 'localhost'\n",
    "PORT = 9998\n",
    "ADDR = (HOST, PORT)\n",
    "tcpSock = socket(AF_INET, SOCK_STREAM)\n",
    "tcpSock.bind(ADDR)\n",
    "tcpSock.listen(5)\n",
    "\n",
    "\n",
    "while True:\n",
    "    c, addr = tcpSock.accept()\n",
    "    print('got connection')\n",
    "    for line in rdd:\n",
    "        try:\n",
    "            c.send(line.encode())\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            break\n",
    "    c.close()\n",
    "    print('disconnected')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
