[
  {
    "objectID": "wyklad2S.html",
    "href": "wyklad2S.html",
    "title": "Wykład 2",
    "section": "",
    "text": "Oczekiwania vs Rzeczywistość\n\nKiedy podjąć decyzję biznesową ?\n\n\n\n\nBatch = Duże, historyczne zbiory\nStream = Strumień danych, on line, przesyłane w trybie ciągłym\n\n\n\n\n\nBatch = minuty, godziny, dni (patrz Hurtownie danych)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = możliwe i stosowane bardzo czesto\nStream = ,,niemożliwe’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)\n\n\n\nGdy mówimy o skali (nie o języku Scala), najczęściej przychodzi nam na myśl przeglądarka Google. Przeszukuje ona ogromne zbiory danychz dużą prędkością. Sama nazwa Goolge wskasuje na skalę (colowo przyjęto błędną nazwę w zapisie googol co oznacza 1 i 100 zer).\n\nSprawdź czy do końca zajęć uda Ci się zapisać liczbę googol na kartce.\n\nPowinno być dla Ciebie jasne, że żadne tradycyjne systemy, np relacyjne systemy baz danych, ani programowanie imperatywne nie są w stanie obłużyć przeszukiwania takiej ilości danych. Problemy te doprowadziły do budowy rozproszonych systemów plików Google File System, MapReduce (paradygmat programowania równoległego), czy Bigtable (skalowalna pamięć masowa ustrukturyzowanych danych znajdujących się na GFS).\n\n\nZnajdź prosty algorytm map reduce w dowolnym języku programowania i uruchom go.\n\n\nJak poprawić ?"
  },
  {
    "objectID": "wyklad2S.html#batch-vs-stream-processing",
    "href": "wyklad2S.html#batch-vs-stream-processing",
    "title": "Wykład 2",
    "section": "",
    "text": "Oczekiwania vs Rzeczywistość\n\nKiedy podjąć decyzję biznesową ?\n\n\n\n\nBatch = Duże, historyczne zbiory\nStream = Strumień danych, on line, przesyłane w trybie ciągłym\n\n\n\n\n\nBatch = minuty, godziny, dni (patrz Hurtownie danych)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = możliwe i stosowane bardzo czesto\nStream = ,,niemożliwe’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)\n\n\n\nGdy mówimy o skali (nie o języku Scala), najczęściej przychodzi nam na myśl przeglądarka Google. Przeszukuje ona ogromne zbiory danychz dużą prędkością. Sama nazwa Goolge wskasuje na skalę (colowo przyjęto błędną nazwę w zapisie googol co oznacza 1 i 100 zer).\n\nSprawdź czy do końca zajęć uda Ci się zapisać liczbę googol na kartce.\n\nPowinno być dla Ciebie jasne, że żadne tradycyjne systemy, np relacyjne systemy baz danych, ani programowanie imperatywne nie są w stanie obłużyć przeszukiwania takiej ilości danych. Problemy te doprowadziły do budowy rozproszonych systemów plików Google File System, MapReduce (paradygmat programowania równoległego), czy Bigtable (skalowalna pamięć masowa ustrukturyzowanych danych znajdujących się na GFS).\n\n\nZnajdź prosty algorytm map reduce w dowolnym języku programowania i uruchom go.\n\n\nJak poprawić ?"
  },
  {
    "objectID": "wyklad2S.html#strumienie-danych",
    "href": "wyklad2S.html#strumienie-danych",
    "title": "Wykład 2",
    "section": "Strumienie danych",
    "text": "Strumienie danych\nStrumieniowanie możesz kojarzyć z serwisów przesyłających video w trybie online. Gdy oglądasz swój ulubiony serial (tak jak teraz na zajęciach) serwis odpowiadający za strumieniowanie w nieprzerwany sposób przesyła do ciebie kolejne “porcje” video. Identycznie koncepcja ta realizowana jest w przypadku danych strumieniowych. Format przesyłanych porcji nie musi być plikiem video, wszystko zależy od celu realizowanego biznesowo. Np. ciągły pomiar z różnego rodzaju czujników w farbykach, elektrowniach itp. Warto odnotować, że masz do czynienia z ciągłym strumieniem danych, które przetwarzać musisz w czasie rzeczywistym. Nie możesz czekać do zatrzymania linii produkcyjnych w celu wykonania analizy, wszystkie pojawiające się problemy chcesz rejestrować natychmiast i jak najszybciej na nie reagować.\n\nAnaliza strumieni danych to ciągłe przetwarzanie i analiza dużych zbiorów danych w ruchu.\n\nPorównuj to do wsakazanych powyżej elementów Big Data. Przetwarzanie Batchowe jest przeciwieństwem do przetwarzania strumieniowego. Najpierw zbierasz duże ilości danych a potem realizujesz analizy. Możesz oczywiście zawsze pobrać video w całości zanim je obejrzysz, ale czy miałoby to sens? Istnieją przypadki gdy takie podejście nie stanowi problemu, ale już tu widzisz, że przetwarzanie strumieniowe może przynieść dla biznesu dodatkowe wartości dodane, których trudno oczekiwać przy wsadowym przetwarzaniu.\nciekawe informacje\n\nŹródła danych przesyłanych strumieniowo obejmują:\n\nczujniki sprzętu,\nstrumienie kliknięć,\nśledzenie lokalizacji\ninterackcja z użytkownikiem: co robią użytkownicy Twojej witryny?\nkanały mediów społecznościowych,\nnotowania giełdowe,\naktywność w aplikacjach\ninne.\n\nFirmy wykorzystują analitykę strumieniową do odkrywania i interpretowania wzorców, tworzenia wizualizacji, przekazywania spostrzeżeń i alertów oraz uruchamiania procesów w czasie rzeczywistym lub zbliżonym do rzeczywistego.\n\n\nPrzykładowe biznesowe zastosowania\n\nDane z sensorów IoT i detekcja anomalii\nStock Trading (problemy regresyjne) - czas reagowania na zmiany i czas zakupy i sprzedaży akcji.\nClickstream for websites (problem klasyfikacji) - śledzenie i analiza gości na stronie serwisu internetowego - personalizacja strony i treści.\n\n8 najlepszych przykładów analizy w czasie rzeczywistym\nBiznesowe zastosowania\n\nPrzedsiębiorstwo to organizacja, która generuje i odpowiada na ciągły strumień zdarzeń.\n\nAnalityka strumieniowa (ang. stream analytics) nazywana jest również przetwarzaniem strumieniowym zdarzeń (ang. event stream processing) - przetwarzanie dużej ilości danych już na etapie ich generowania.\nNiezależnie od zastosowanej technologi wszystkie dane powstają jako ciągły strumień zdarzeń (działania użytkowników na stronie www, logi systemowe, pomiary z sensorów)."
  },
  {
    "objectID": "wyklad2S.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "href": "wyklad2S.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "title": "Wykład 2",
    "section": "Czas w analizie danych w czasie rzeczywistym",
    "text": "Czas w analizie danych w czasie rzeczywistym\nW przypadku przetwarzania wsadowego przetwarzamy dane historyczne i czas uruchomienia procesu przetwarzania nie ma nic wspólnego z czasem występowania analizowanych zdarzeń.\nDla danych strumieniowych mamy dwie koncepcje czasu:\n\nczas zdarzenia (event time) - czas w którym zdarzenie się wydarzyło.\nczas przetwarzania (processing time) - czas w którym system przetwarza zdarzenie.\n\nW przypadku idealnej sytuacji:\n\nW rzeczywistości przetwarzanie danych zawsze odbywa się z pewnym opóźnieniem, co reprezentowane jest przez punkty pojawiające się poniżej funkcji dla sytuacji idealnej (poniżej diagonalnej).\n\nW aplikacjach przetwarzania strumieniowego istotne okazują się różnice miedzy czasem powstania zdarzenia i jego procesowania. Do najczęstszych przyczyn opóźnienia wyszczególnia się przesyłanie danych przez sieć czy brak komunikacji między urządzeniem a siecią. Prostym przykładem jest tu przejazd samochodem przez tunel i śledzenie położenia przez aplikację GPS.\nMożesz oczywiście zliczać ilość takich pominiętych zdarzeń i uruchomić alarm w sytuacji gdy takich odrzutów będzie za dużo. Drugim (chyba częściej) wykorzystywanym sposobem jest zastosowanie korekty z wykorzystaniem tzw. watermarkingu.\nProces przetwarzania zdarzeń w czasie rzeczywistym można przedstawić w postaci funkcji schodkowej, reprezentowanej na rysunku: \nJak można zauważyć nie wszystkie zdarzenia wnoszą wkład do analizy i przetwarzania. Realizację procesu przetwarzania wraz z uwzględnieniem dodatkowego czasu na pojawienie się zdarzeń (watermark) można przedstawić jako proces obejmujący wszystkie zdarzenia powyżej przerywanej linii. Dodatkowy czas pozwolił na przetworzenie dodatkowych zdarzeń, natomiast nadal mogą zdarzyć się punkty, które nie będą brane pod uwagę.  \nPrzedstawione na wykresach sytuacje jawnie wskazują dlaczego pojęcie czasu jest istotnym czynnikiem i wymaga ścisłego określenia już na poziomie definiowania potrzeb biznesowych. Przypisywanie znaczników czasu do danych (zdarzeń) to trudne zadanie."
  },
  {
    "objectID": "wyklad2S.html#okna-czasowe",
    "href": "wyklad2S.html#okna-czasowe",
    "title": "Wykład 2",
    "section": "okna czasowe",
    "text": "okna czasowe\nOkno rozłączne (ang. tumbling window) czyli okno o stałej długości. Jego cechą charakterystyczną jest to, iż każde zdarzenie należy tylko do jednego okna.  \nOkno przesuwne (ang. sliding window) obejmuje wszystkie zdarzenia następujące w określonej długości między sobą.  \nOkno skokowe (ang. hopping window) tak jak okno rozłączne ma stałą długość, ale pozwala się w nim na zachodzenie jednych okien na inne. Stosowane zazwyczaj do wygładzenia danych.  \nKomunikacja sieciowa, relacyjne bazy danych, rozwiązania chmurowe i big data znacząco zmieniły sposób budowania systemów informatycznych i wykonywnia na niach pracy.\nPorównaj to jak “narzędzia” do realizacji przekazu (gazeta, radio, telewizja, internet, komunikatory, media społecznościowe) zmieniły interakcje międzyludzkie i struktury społeczne.\n\nKażde nowe informatyczne medium zmieniło stosunek ludzi do informatyki.\n\nKoncepcja mikrousługi (mikroserwisu) jest bardzo popularnym sposobem budowania systemów informatycznych jak i koncepcją przy tworzeniu oprogramowania czy realizacji firmy w duchu Data-Driven. Koncepcja ta pozwala zachować wydajność (rób jedną rzecz ale dobrze), elastyczność i jasną postać całej struktury.\nChociaż istnieją inne sposoby architektury projektów oprogramowania, „mikroserwisy” są często używane nie bez powodu. Idea mikroserwisów tkwi w nazwie: oprogramowanie jest reprezentowane jako wiele małych usług, które działają indywidualnie. Patrząc na ogólną architekturę, każda mikrousługa znajduje się w małej czarnej skrzynce z jasno zdefiniowanymi wejściami i wyjściami. Możesz porównać tego typu zachowanie do “czystej funkcji” w programowaniu funkcyjnym.\nW celu umożliwienia komunikacji różnych mikroserwisów często wybieranym rozwiązaniem jest wykorzystanie Application Programming Interfaces API .\n\nKomunikacja przez API\nCentralnym elementem architektury mikrousług jest wykorzystanie interfejsów API. API to część, która pozwala na połączenie dwóch mikroserwisów. Interfejsy API są bardzo podobne do stron internetowych. Podobnie jak strona internetowa, serwer wysyła do Ciebie kod reprezentujący stronę internetową. Twoja przeglądarka internetowa interpretuje ten kod i wyświetla stronę internetową.\nWeźmy przypadek biznesowy z modelem ML jako usługą. Załóżmy, że pracujesz dla firmy sprzedającej mieszkania w Bostonie. Chcesz zwiększać sprzedaż i oferować naszym klientom lepszą jakość usług dzięki nowej aplikacji mobilnej, z której może korzystać nawet 1 000 000 osób jednocześnie. Możemy to osiągnąć, udostępniając prognozę wartości domu, gdy użytkownik prosi o wycenę przez Internet.\n\nCzym jest serwowanie modelu ML\n\nSzkolenie dobrego modelu ML to TYLKO pierwsza część całego procesu: Musisz udostępnić swój model użytkownikom końcowym. Robisz to, zapewniając dostęp do modelu na swoim serwerze.\nAby udostępnić model potrzebujesz: modelu, interpretera, danych wsadowych.\nWażne metryki\n\n\nczas oczekiwania,\nkoszty,\nliczba zapytać w jednostce czasu\n\n\nUdostępnianie danych między dwoma lub więcej systemami zawsze było podstawowym wymogiem tworzenia oprogramowania – DevOps vs. MLOps.\n\nGdy wywołasz interfejs API, otrzyma on Twoje żądanie. Żądanie wyzwala kod do uruchomienia na serwerze i generuje odpowiedź odesłaną do Ciebie. Jeśli coś pójdzie nie tak, możesz nie otrzymać żadnej odpowiedzi lub otrzymać kod błędu jako kod stanu HTTP.\n\nKlient-Serwer: Klient (system A) przesyła żądanie przez HTTP do adresu URL hostowanego przez system B, który zwraca odpowiedź. Identycznie działa np przeglądarka internetowa. Żądanie jest kierowane do serwera WWW, który zwraca tekstową stronę HTML.\n\n\nBezstanowe: Żądanie klienta powinno zawierać wszystkie informacje niezbędne do udzielenia pełnej odpowiedzi.\n\nInterfejsy API można wywoływać za pomocą wielu różnych narzędzi. Czasami możesz nawet użyć przeglądarki internetowej. Narzędzia takie jak CURL wykonują zadanie w wierszu poleceń. Możesz używać narzędzi, takich jak Postman, do wywoływania interfejsów API za pomocą interfejsu użytkownika.\n\nCała komunikacja jest objęta ustalonymi zasadami i praktykami, które są nazywane protokołem HTTP.\n\n\n\nZapytanie - Request\n\nAdres URL (np. http://mydomain:8000/getapi?&val1=43&val2=3) zawiera: - domenę, - port, - dodatkowe ścieżki, - zapytanie\nMetody HTTP: - GET, - POST\nNagłówki HTTP zawierają: - informacje o autoryzacji, - cookies metadata Cała informacja zawarta jest w Content-Type: application/json, text … Accept: application/json, Authorization: Basic abase64string, Tokens\nCiało zapytania\n\nNajczęściej wybieranym formatem dla wymiany informacji między serwisami jest format JavaScript Object Notation (JSON). Przypomina on pythonowy obiekt słownika - “klucz”: “wartość”.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nOdpowiedź - Response\n\nTreść odpowiedzi przekazywana jest razem z nagłówkiem oraz statusem:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nnp.: “Content-Type” =&gt; ”application/json; charset=utf-8”, ”Server” =&gt; ”Genie/Julia/1.8.5”\nTreść (ciało) odpowiedzi:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code: • 200 OK - prawidłowe wykonanie zapytania, • 40X Access Denied • 50X Internal server error\n\n\nWyszukaj informacje czym jest REST API.\n\n\nWiedza:\n\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie\n\n\n\nUmiejętności:\n\nRozumie ograniczenia wynikające z czasu przetwarzania przez urządzenia oraz systemy informatyczne\n\n\n\nKompetencje:\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\nFormułuje problem analityczny wraz z jego informatycznym rozwiązaniem"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr letni 2024, SGH Szkoła Główna Handlowa w Warszawie\nPodstawowe informacje znajdziesz w sylabusie.\nCiekawe książki i strony internetowe zamieszczone zostały w zakładce książki. Jeśli chciał(a)byś coś dodać prześlij informację przez MS teams.",
    "crumbs": [
      "222890-D",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "index.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-D\nSemestr letni 2024, SGH Szkoła Główna Handlowa w Warszawie\nPodstawowe informacje znajdziesz w sylabusie.\nCiekawe książki i strony internetowe zamieszczone zostały w zakładce książki. Jeśli chciał(a)byś coś dodać prześlij informację przez MS teams.",
    "crumbs": [
      "222890-D",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\nWykład jest realizowany w trybie stacjonarnym. Jest on NIEOBOWIĄZKOWY i odbywa się w Auli II bud G\n\n\n26-02-2024 (poniedziałek) 09:50-11:30 - Wykład 1\n\nTematy zrealizowane na wykładzie:\n\n\nDane ustrukturyzowane i nieustrukturyzowane\nProcesy generowania danych\nBig Data\n\n\n\n04-03-2024 (poniedziałek) 09:50-11:30 - Wykład 2\n\nTematy zrealizowane:\n\n\nModele przetwarzania danych OLTP, OLAP\nSzybkość podejmowania decyzji\nDefinicje eventów, strumieni danych\nczas w strumieniach danych\n\n\n\n11-03-2024 (poniedziałek) 09:50-11:30 - Wykład 3\n\nTematy zrealizowane:\n\n18-03-2024 (poniedziałek) 09:50-11:30 - Wykład 4\n\n\n25-03-2024 (poniedziałek) 09:50-11:30 - Wykład 5\n\n\n\nTEST 20 pytań! - 30 minut.\nPrzygotowanie środowiska pracy\n\nTest przeprowadzany jest za pośrednictwem MS Teams.\n\n\nLaboratoria\n\n\n08-04-2024 (poniedziałek) 08:00-13:20 - C2D 3 grupy\n\n\n09-04-2024 (wtorek) 09:50-17:00 - Sabinki 16 3 grupy\n\n\n\nWprowadzenie do środowiska Python - API FLask\n\n\n\n15-04-2024 (poniedziałek) 08:00-13:30 - C2D, 3 grupy\n\n\n16-04-2024 (wtorek) 09:50-17:00 - Sabinki 16, 3 grupy\n\n\n\nAPI Flask kontrola zapytań i odpowiedzi\nProducent Apache Kafka\n\n\n\n22-04-2024 (poniedziałek) 08:00-13:30 - C2D, 3 grupy\n\n\n23-04-2024 (wtorek) 09:50-17:00 - Sabinki 16, 3 grupy\n\n\n\nDane ustrukturyzowane i ich modelowanie\nZadania: czyszczenie danych tabelarycznych\n\n\n\n29-04-2024 (poniedziałek) 08:00-13:30 - C2D, 3 grupy\n\n\n30-04-2024 (wtorek) 09:50-17:00 - Sabinki 16, 3 grupy\n\n\n\nGodziny rektorskie\n\n\n06-05-2024 (poniedziałek) 08:00-13:30 - C2D, 3 grupy\n07-05-2024 (wtorek) 09:50-17:00 - Sabinki 16, 3 grupy\n13-05-2024 (poniedziałek) 08:00-13:30 - C2B, 3 grupy\n14-05-2024 (wtorek) 09:50-17:00 - Sabinki 16, 3 grupy\n20-05-2024 (poniedziałek) 08:00-13:30 - C2B, 3 grupy\n21-05-2024 (wtorek) 09:50-17:00 - Sabinki 16, 3 grupy\n27-05-2024 (poniedziałek) 08:00-13:30 - C2B, 3 grupy\n28-05-2024 (wtorek) 09:50-17:00 - Sabinki 16, 3 grupy\n03-06-2024 (poniedziałek) 08:00-13:30 - C2B, 3 grupy\n04-06-2024 (wtorek) 11:40-17:00 - Sabinki 16, 3 grupy\n\n\n\n\nMiejsce\nWykłady 1-5: G-Aula II Laboratorium 1-9: C2D, Sabinki 16\n\n\nZaliczenie i Egzamin\nWykłady zakończone zostaną testem (ostatnie zajęcia). Pozytywna ocena z testu (powyżej 13 pkt) upoważnia do realizacji ćwiczeń.\nPo ćwiczeniach realizowane będą zadania domowe przekazywane za pośrednictwem platformy teams.\nZaliczenie wszystkich ćwiczeń i zadań upoważnia do realizacji projektu.\nProjekt powinien być realizowany w grupach max 5 osobowych.\nWymagania projektu:\n\nProjekt powinien przedstawiać BIZNESOWY PROBLEM, który można realizować wykorzystując informacje podawane w trybie online. (Nie oznacza to, że nie można korzystać z procesowania batchowego np w celu wygenerowania modelu).\nDane powinny być przesyłane do Apache Kafki i stamtąd poddawane dalszemu procesowaniu i analizie.\nJęzyk programowania jest dowolny - dotyczy każdego komponentu projektu.\nMożna wykorzystać narzędzia BI\nŹródłem danych może być tabela, sztucznie generowane dane, IoT itp.",
    "crumbs": [
      "222890-D",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Flink, Apache Kafka, Apache Beam\nDatabricks Community edition Web page.",
    "crumbs": [
      "222890-D",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nJeśli zadne z powyzszych poleceń nie działa, oznacza to, ze nie posiadasz interpretera python na swoim komputerze i musisz go doinstalować.\nSprawdź równiez polecenia\npython -m pip --version\npython -m pip3 --version\nPolecenia pip słuzą do instalacji pakietów.\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\nWarto zweryfikować równiez informację do którego środowiska pythona odnosi się poszczególna komenda.\n\n\nDo efektywnej pracy nad dowolnym projektem, zaleca się utworzenie dedykowanego środowiska wirtualnego z bibliotekami dostosowanymi do potrzeb konkretnego projektu. Takie podejście pozwala uniknąć problemów związanych z nadpisywaniem bibliotek.\npython3 -m venv venv\n# uruchom środowisko\nsource venv/bin/activate\n# . env/bin/activate\n\n(venv)$ \nW powyzszym przykładzie wykorzystaliśmy pakiet venv (python -m venv) do utworzenia nowego środowiska (katalogu) o nazwie venv.\nUruchomienie środowiska odbywa się z wykorzystaniem komendy source (lub .).\nPamiętaj, ze w przypadku systemu Windows środowisko uruchamia się przez\nvenv/Scripts/activate\nPosiadając nowe (puste) środowisko wykonaj następujące czynności: 1. Zainstaluj nowe wersje podstawowych pakietów\npip install --no-cache --upgrade pip setuptools\nSzybka instalacja podstawowych bibliotek i jupyter notebook.\n\npip install notebook numpy pandas matplotlib scipy\n\n# uruchom \njupyter notebook\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyter.\nsource &lt;name of env&gt;/bin/activate\njupyter notebook\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nJeśli zadne z powyzszych poleceń nie działa, oznacza to, ze nie posiadasz interpretera python na swoim komputerze i musisz go doinstalować.\nSprawdź równiez polecenia\npython -m pip --version\npython -m pip3 --version\nPolecenia pip słuzą do instalacji pakietów.\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\nWarto zweryfikować równiez informację do którego środowiska pythona odnosi się poszczególna komenda.\n\n\nDo efektywnej pracy nad dowolnym projektem, zaleca się utworzenie dedykowanego środowiska wirtualnego z bibliotekami dostosowanymi do potrzeb konkretnego projektu. Takie podejście pozwala uniknąć problemów związanych z nadpisywaniem bibliotek.\npython3 -m venv venv\n# uruchom środowisko\nsource venv/bin/activate\n# . env/bin/activate\n\n(venv)$ \nW powyzszym przykładzie wykorzystaliśmy pakiet venv (python -m venv) do utworzenia nowego środowiska (katalogu) o nazwie venv.\nUruchomienie środowiska odbywa się z wykorzystaniem komendy source (lub .).\nPamiętaj, ze w przypadku systemu Windows środowisko uruchamia się przez\nvenv/Scripts/activate\nPosiadając nowe (puste) środowisko wykonaj następujące czynności: 1. Zainstaluj nowe wersje podstawowych pakietów\npip install --no-cache --upgrade pip setuptools\nSzybka instalacja podstawowych bibliotek i jupyter notebook.\n\npip install notebook numpy pandas matplotlib scipy\n\n# uruchom \njupyter notebook\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyter.\nsource &lt;name of env&gt;/bin/activate\njupyter notebook\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-serwisu-github",
    "href": "info.html#zacznij-korzystać-z-serwisu-github",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z serwisu GitHub",
    "text": "Zacznij korzystać z serwisu GitHub\n\n\n\nTekst na podstawie strony jak korzystać z serwisu github\nPracując nad projektem np. praca magisterska, (samodzielnie lub w zespole) często potrzebujesz sprawdzić jakie zmiany, kiedy i przez kogo zostały wprowadzone do projektu. W zadaniu tym świetnie sprawdza się system kontroli wersji czyli GIT.\nGit możesz pobrać i zainstalować jak zwykły program na dowolnym komputerze. Jednak najczęściej (małe projekty) korzysta się z serwisów z jakimś systemem git. Jednym z najbardziej rozpoznawanych jest GitHub dzięki któremu możesz korzystać z systemu git bez jego instalacji na swoim komputerze.\nW darmowej wersji serwisu GitHub swoje pliki możesz przechowywać w publicznych (dostęp mają wszyscy) repozytoriach.\nSkupimy się wyłącznie na darmowej wersji serwisu GitHub.\ngit --version\n\nStruktura GitHuba\nNa najwyższym poziomie znajdują się konta indywidualne (np http://github.com/sebkaz, bądź zakładane przez organizacje. Użytkownicy indywidualni mogą tworzyć repozytoria publiczne (public ) bądź prywatne (private).\nJeden plik nie powinien przekraczać 100 MB.\nRepo (skrót do repozytorium) tworzymy za pomocą Create a new repository. Każde repo powinno mieć swoją indywidualną nazwę.\n\n\nBranche\nGłówna (tworzona domyślnie) gałąź rapozytorium ma nazwę master.\n\n\nNajważniejsze polecnia do zapamiętania\n\nściąganie repozytorium z sieci\n\ngit clone https://adres_repo.git\n\nW przypadku githuba możesz pobrać repozytorium jako plik zip.\n\n\nTworzenie repozytorium dla lokalnego katalogu\n\n# tworzenie nowego katalogu\nmkdir datamining\n# przejście do katalogu\ncd datamining\n# inicjalizacja repozytorium w katalogu\ngit init\n# powinien pojawić się ukryty katalog .git\n# dodajmy plik\necho \"Info \" &gt;&gt; README.md\n\nPołącz lokalne repozytorium z kontem na githubie\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\nObsługa w 3 krokach\n\n# sprawdź zmiany jakie zostały dokonane\ngit status\n# 1. dodaj wszystkie zmiany\ngit add .\n# 2. zapisz bierzący stan wraz z informacją co zrobiłeś\ngit commit -m \" opis \"\n# 3. potem już zostaje tylko\ngit push origin master\nWarto obejrzeć Youtube course.\nCiekawe i proste wprowadzenie mozna znaleźć tutaj"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-dockera",
    "href": "info.html#zacznij-korzystać-z-dockera",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z Dockera",
    "text": "Zacznij korzystać z Dockera\n\n\n\nW celu pobrania oprogramowania docker na swój system przejdź do strony.\nJeżli wszystko zainstalowało się prawidłowo wykonaj następujące polecenia:\n\nSprawdź zainstalowaną wersję\n\ndocker --version\n\nŚciągnij i uruchom obraz Hello World i\n\ndocker run hello-world\n\nPrzegląd ściągnietych obrazów:\n\ndocker image ls\n\ndocker images\n\nPrzegląd uruchomionych kontenerów:\n\ndocker ps \n\ndocker ps -all\n\nZatrzymanie uruchomionego kontenera:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nUsunięcie kontenera\n\ndocker rm -f &lt;CONTAINER ID&gt;\nPolecam również krótkie intro"
  },
  {
    "objectID": "lab/cw3.html",
    "href": "lab/cw3.html",
    "title": "Dane ustrukturyzowane",
    "section": "",
    "text": "# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "lab/cw3.html#dane-jako-zmienne",
    "href": "lab/cw3.html#dane-jako-zmienne",
    "title": "Dane ustrukturyzowane",
    "section": "Dane jako zmienne",
    "text": "Dane jako zmienne\n\n# variables\ncustomer1_age = 38\ncustomer1_height = 178\ncustomer1_loan = 34.23\ncustomer1_name = 'Zajac'\n\n\ndlaczego do analizy danych nie używamy zmiennych?\nNiezależnie od typu analizowanych i przetwarzanych danych w Pythonie możemy zebrać dane i reprezentować je jako pewna formy listy.\n\n\n# python lists\ncustomer = [38, 'Divorced', 1, 56.3, [\"\",\"\",\"\"], {}]\nprint(customer)\n\n\n# different types in one object\ntype(customer)\n\n\ndlaczego listy nie są najlepszym miejscem na przechowywanie danych?\n\nWeźmy dwie listy numeryczne\n\n# dwie listy danych\na = [1,2,3]\nb = [4,5,6]\n\nTypowe operacje na listach w analizach danych\n\n# dodawanie list\nprint(f\"a+b: {a+b}\")\n# można też użyć metody format\nprint(\"a+b: {}\".format(a+b))\n\n\n# mnożenie list\ntry:\n    print(a*b)\nexcept TypeError:\n    print(\"no-defined operation\")\n\nKażdy obiekt pythonowy można rozszerzyć o nowe metody i atrybuty.\n\nimport numpy as np\naa = np.array(a)\nbb = np.array(b)\n\nprint(aa,bb)\n\n\nprint(f\"aa+bb: {aa+bb}\")\n# dodawanie działa\ntry:\n    print(\"=\"*50)\n    print(aa*bb)\n    print(\"aa*bb - czy to poprawne mnożenie?\")\n    print(np.dot(aa,bb))\n    print(\"np.dot - a czy otrzymany wynik też realizuje poprawne mnożenie?\")\nexcept TypeError:\n    print(\"no-defined operation\")\n# mnożenie również działa\n\n\n# własności tablic\nx = np.array(range(4))\nprint(x)\nx.shape\n\n\nA = np.array([range(4),range(4)])\n# transposition  row i -&gt; column j, column j -&gt; row i \nA.T\n\n\n# 0-dim object\nscalar = np.array(5)\nprint(f\"scalar object dim: {scalar.ndim}\")\n# 1-dim object\nvector_1d = np.array([3, 5, 7])\nprint(f\"vector object dim: {vector_1d.ndim}\")\n# 2 rows for 3 features\nmatrix_2d = np.array([[1,2,3],[3,4,5]])\nprint(f\"matrix object dim: {matrix_2d.ndim}\")\n\n\nKurs Numpy ze strony Sebastiana Raschki",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "lab/cw3.html#pytorch",
    "href": "lab/cw3.html#pytorch",
    "title": "Dane ustrukturyzowane",
    "section": "PyTorch",
    "text": "PyTorch\nPyTorch is an open-source Python-based deep learning library. PyTorch has been the most widely used deep learning library for research since 2019 by a wide margin. In short, for many practitioners and researchers, PyTorch offers just the right balance between usability and features.\n\nPyTorch is a tensor library that extends the concept of array-oriented programming library NumPy with the additional feature of accelerated computation on GPUs, thus providing a seamless switch between CPUs and GPUs.\nPyTorch is an automatic differentiation engine, also known as autograd, which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization.\nPyTorch is a deep learning library, meaning that it offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models, catering to both researchers and developers.\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\n\ntensor0d = torch.tensor(1) \ntensor1d = torch.tensor([1, 2, 3])\ntensor2d = torch.tensor([[1, 2, 2], [3, 4, 5]])\ntensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n\nprint(tensor1d.dtype)\n\n\ntorch.tensor([1.0, 2.0, 3.0]).dtype\n\n\ntensor2d\n\n\ntensor2d.shape\n\n\nprint(tensor2d.reshape(3, 2))\n\n\nprint(tensor2d.T)\n\n\nprint(tensor2d.matmul(tensor2d.T))\n\n\nprint(tensor2d @ tensor2d.T)\n\nszczegółowe info znajdziesz w dokumentacji",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "lab/cw3.html#modelowanie-danych-ustrukturyzowanych",
    "href": "lab/cw3.html#modelowanie-danych-ustrukturyzowanych",
    "title": "Dane ustrukturyzowane",
    "section": "Modelowanie danych ustrukturyzowanych",
    "text": "Modelowanie danych ustrukturyzowanych\nRozważmy jedną zmienną (xs) od której zależy nasza zmienna wynikowa (ys - target).\nxs = np.array([-1,0,1,2,3,4])\nys = np.array([-3,-1,1,3,5,7])\nModelem który możemy zastosować jest regresja liniowa.\n\n# Regresja liniowa \n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nxs = np.array([-1,0,1,2,3,4])\n# a raczej \nxs = xs.reshape(-1, 1)\n\nys = np.array([-3, -1, 1, 3, 5, 7])\n\nreg = LinearRegression()\nmodel = reg.fit(xs,ys)\n\nprint(f\"solution: x1={model.coef_[0]}, x0={reg.intercept_}\")\n\nmodel.predict(np.array([[1],[5]]))\n\nProsty kod realizuje w pełni nasze zadanie znalezienia modelu regresji liniowej.\nDo czego może nam posłużyc tak wygenerowany model?\nAby z niego skorzystac potrzebujemy wyeksportować go do pliku.\n\n# save model\nimport pickle\nwith open('model.pkl', \"wb\") as picklefile:\n    pickle.dump(model, picklefile)\n\nTeraz możemy go zaimportować (np na Github) i wykorzystać w innych projektach.\n\n# load model\nwith open('model.pkl',\"rb\") as picklefile:\n    mreg = pickle.load(picklefile)\n\nAle !!! pamiętaj o odtworzeniu środowiska Pythonowego\n\nmreg.predict(xs)",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "lab/cw3.html#siecie-neuronowe",
    "href": "lab/cw3.html#siecie-neuronowe",
    "title": "Dane ustrukturyzowane",
    "section": "siecie neuronowe",
    "text": "siecie neuronowe\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\n\nimport tensorflow as tf\n\nNa ten problem możemy popatrzeć z innej perspektywy. Sieci neuronowe również potrafią rozwiązywać problemy regresji.\n\nlayer_0 = Dense(units=1, input_shape=[1])\n\nmodel = Sequential([layer_0])\n\n# compilowanie i fitowanie\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\nmodel.fit(xs, ys, epochs=10)\n\n\nprint(f\"{layer_0.get_weights()}\")",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "lab/cw3.html#inne-sposoby-pozyskiwania-danych",
    "href": "lab/cw3.html#inne-sposoby-pozyskiwania-danych",
    "title": "Dane ustrukturyzowane",
    "section": "Inne sposoby pozyskiwania danych",
    "text": "Inne sposoby pozyskiwania danych\n\nGotowe źródła w bibliotekach pythonowych\nDane z plików zewnętrznych (np. csv, json, txt) z lokalnego dysku lub z internetu\nDane z bazy danych (np. MySQL, PostgreSQL, MongoDB)\nDane generowane w sposób sztuczny pod wybrany problem modelowy.\nStrumienie danych\n\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n\n\n# find all keys\niris.keys()\n\n\n# print description\nprint(iris.DESCR)\n\n\nimport pandas as pd\nimport numpy as np\n\n# create DataFrame\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                  columns= iris['feature_names'] + ['target'])\n\n\n# show last\ndf.tail(10)\n\n\n# show info about NaN values and a type of each column.\ndf.info()\n\n\n# statistics\ndf.describe()\n\n\n# new features\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n\n# remove features (columns) \ndf = df.drop(columns=['target'])\n# filtering first 100 rows and 4'th column\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\", palette=\"husl\")\n\niris_melt = pd.melt(df, \"species\", var_name=\"measurement\")\nf, ax = plt.subplots(1, figsize=(15,9))\nsns.stripplot(x=\"measurement\", y=\"value\", hue=\"species\", data=iris_melt, jitter=True, edgecolor=\"white\", ax=ax)\n\n\nX = df.iloc[:100,[0,2]].values\ny = df.iloc[0:100,4].values\n\n\ny = np.where(y == 'setosa',-1,1)\n\n\nplt.scatter(X[:50,0],X[:50,1],color='red', marker='o',label='setosa')\nplt.scatter(X[50:100,0],X[50:100,1],color='blue', marker='x',label='versicolor')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\nDla tego typu danych separowalnych liniowo użyj modelu regresji logistycznej lub sieci neuronowej.\n\nfrom sklearn.linear_model import Perceptron\n\nper_clf = Perceptron()\nper_clf.fit(X,y)\n\ny_pred = per_clf.predict([[2, 0.5],[4,5.5]])\ny_pred",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "lab/cw3.html#zapis-danych-i-podłączenie-do-prostej-bazy-sql",
    "href": "lab/cw3.html#zapis-danych-i-podłączenie-do-prostej-bazy-sql",
    "title": "Dane ustrukturyzowane",
    "section": "Zapis danych i podłączenie do prostej bazy SQL",
    "text": "Zapis danych i podłączenie do prostej bazy SQL\n\nIRIS_PATH = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\ncol_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\ndf = pd.read_csv(IRIS_PATH, names=col_names)\n\n\n# save to sqlite\nimport sqlite3\n# generate database\nconn = sqlite3.connect(\"iris.db\")\n# pandas to_sql\n\ntry:\n    df.to_sql(\"iris\", conn, index=False)\nexcept:\n    print(\"tabela już istnieje\")\n\n\n# sql to pandas\nresult = pd.read_sql(\"SELECT * FROM iris WHERE sepal_length &gt; 5\", conn)\n\n\nresult.head(3)\n\n\n# Dane sztucznie generowane\nfrom sklearn import datasets\nX, y = datasets.make_classification(n_samples=10**4,\nn_features=20, n_informative=2, n_redundant=2)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# podział na zbiór treningowy i testowy\ntrain_samples = 7000 # 70% danych treningowych\n\nX_train = X[:train_samples]\nX_test = X[train_samples:]\ny_train = y[:train_samples]\ny_test = y[train_samples:]\n\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\n\nrfc.predict(X_train[0].reshape(1, -1))",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "lab/cw3.html#zadania",
    "href": "lab/cw3.html#zadania",
    "title": "Dane ustrukturyzowane",
    "section": "ZADANIA",
    "text": "ZADANIA\n\nZaładuj plik z danymi train.csv do pandasowej ramki danych o nazwie df\n\n\n## YOUR CODE HERE\ndf = \n\n\nWypisz liczbę wierszy i kolumn załadowanej ramki\n\n\n## YOUR CODE HERE\n\n\nDokonaj czyszczenia braków danych:\n\nopcja 1 - usuń wiersze zawierające brak danych (dropna())\nopcja 2 - usuń kolumny zawierające brak danych (drop())\nopcja 3 - dokonaj imputacji za pomocą wartości średniej (fillna())\n\n\nKtóre kolumny wybrałeś(aś) do realizacji poszczególnej opcji i dlaczego?\n\n## YOUR CODE HERE\n\n\nKorzystając z metody nunique() usuń kolumny, które nie nadają się do modelowania.\n\n\n## YOUR CODE HERE\n\n\nZamień zmienne kategoryjne z wykorzystaniem LabelEncoder na postać numeryczną\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n## YOUR CODE HERE\n\n\nWykorzystaj MinMaxScaler do transformacji danych zmiennoprzecinkowych do wspólnej skali\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n## YOUR CODE HERE\n\n\nPodziel dane na zbiór treningowy (0.8) i testowy (0.2)\n\n\nfrom sklearn.model_selection import train_test_split\n## YOUR CODE HERE\nX_train, X_test, y_train, y_test = train_test_split(...., random_state=44)\n\n\nWykorzystując mapowanie możesz dla każdego pasażera przeprowadzić klasyfikację. Funkcja run() wymaga podania klasyfikatora dla pojedynczego przypadku.\n\nNapisz klasyfikator przypisujący wartość 0 lub 1 w sposób losowy (możesz wykorzystać funkcję random.randint(0,1)).\nWykonaj fukncję evaluate() i sprawdź jak dobrze radzi sobie losowy klasyfikator.\n\n\n\nclassify = ...\n\n\ndef run(f_classify, x):\n    return list(map(f_classify, x))\n\ndef evaluate(predictions, actual):\n    correct = list(filter(\n        lambda item: item[0] == item[1],\n        list(zip(predictions, actual))\n    ))\n    return f\"{len(correct)} poprawnych przewidywan z {len(actual)}. Accuracy ({len(correct)/len(actual)*100:.0f}%)\"\n\n\nevaluate(run(classify, X_train.values), y_train.values)",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Dane ustrukturyzowane"
    ]
  },
  {
    "objectID": "wyklad1.html",
    "href": "wyklad1.html",
    "title": "Od plików płaskich do Data Mash",
    "section": "",
    "text": "Rozwój technologii informatycznych spowodował dostęp do niewyobrażalnych ilości nowego zasobu jakim są ustrukturyzowane jak i nieustrukturyzowane dane.\nDane przyczyniły się do powstania tysięcy nowych narzędzi do generowania, zbierania, przechowywania i przetwarzania informacji na niespotykaną dotąd skalę.\nZasób ten nie jest nowością i dostępny jest od bardzo dawna. Jednak dopiero po wprowadzeniu systemu pisma można było zacząć prowadzić zapis i przetwarzanie w postaci rachunkowości czy rejestrów różnych rzeczy takich jak: zaludnienie w krajach, spisy rzek, jezior, najgłębsze miejsca itp.\nPojawienie się nowych wyzwań naukowych i biznesowych staje się możliwe do realizacji dzięki budowie systemów opartych na otwartym oprogramowaniu, jak również dzięki wykorzystaniu klastrów komputerów do wspomagania przetwarzania ogromnych ilości danych.\nDziś systemy takie jak SAS, Apache Hadoop,Apache Kafka , Apache Spark, Apache Flink i ich chmurowe odpowiedniki, używane są na szeroką skalę w wielu instytucjach i firmach niemal w każdej dziedzinie. Narzędzia te wykorzystywane są w bankowości, opiece zdrowotnej, naukach przyrodniczych, produkcji, sektorze publicznym czy sprzedaży.\nEpoka danych stawia przed nami coraz to nowsze wyzwania związane nie tylko z ilością, ale i z czasem przetwarzania danych.\nNowe wyzwania biznesowe to między innymi:\n\ninteligentna reklama tysięcy produktów dla milionów klientów,\nprzetwarzanie danych o genach, RNA czy też białkach genus,\ninteligentne wykrywanie różnorodnych sposobów nadużyć wśród setek miliardów transakcji kart kredytowych,\nsymulacje giełdowe oparte o tysiące instrumentów finansowych\n…\n\n\n\nDane ustrukturyzowane zorganizowane są w kolumnach cech charakteryzujących każdą obserwację (wiersze). Kolumny posiadają etykietę, która wskazuje na ich interpretację.\nPrzykładem kolumn mogą być takie cechy jak: płeć, wzrost czy ilość kedytów, na podstawie których można przewidywać czy klient będzie spłacał kredyt czy też nie.\n\n\nCode\ndane_klientow = {\"sex\":[\"m\",\"f\",\"m\",\"m\",\"f\"],\n \"height\":[160, 172, 158, 174, 192],\n \"credits\":[0,0,1,3,1]\n }\n\ndf = pd.DataFrame(dane_klientow)\nprint(df)\n\n\n  sex  height  credits\n0   m     160        0\n1   f     172        0\n2   m     158        1\n3   m     174        3\n4   f     192        1\n\n\nTakie przewidywanie również oznaczane jest jako cecha (ang. target).\n\n\nCode\ndf['target'] = [0,1,1,0,0]\nprint(df)\n\n\n  sex  height  credits  target\n0   m     160        0       0\n1   f     172        0       1\n2   m     158        1       1\n3   m     174        3       0\n4   f     192        1       0\n\n\n\n\n\nW wielu językach programowania domyślnym pojemnikiem na przechowywanie wartości są zmienne.\nwiek = 47\nstan_cywilny = 'kawaler'\nNie są one jednak praktyczną formą do przechowywania i manipulowania danymi.\nJednym z rozwiązań jest przechowywanie wszystkich cech (np. klienta) w jednym obiekcie.\nW Pythonie zadanie to moze realizować obiekt listy, który pozwala przechowywać rózne typy danych w jednym obiekcie.\n\nklient = [38, 'kawaler', 1, 56.3]\nprint(f\"dane klienta {klient} w obiekcie: {type(klient)}\")\n\ndane klienta [38, 'kawaler', 1, 56.3] w obiekcie: &lt;class 'list'&gt;\n\n\nZ punktu widzenia przerwarzania i modelowania mozliwość ta wprowadza więcej problemów niz korzyści. Sprawdźmy domyślne operacje:\n\n\nCode\na = [1,2,3]\nb = [4,5,6]\nprint(f\"a={a}, b={b}\")\nprint(f\"a+b={a+b}\")\n\n\na=[1, 2, 3], b=[4, 5, 6]\na+b=[1, 2, 3, 4, 5, 6]\n\n\nnatomiast:\n\n\nCode\nprint(f\"a={a}, b={b}\")\nprint(f\"a*b\")\ntry:\n    print(f\"a*b= {a*b}\")\nexcept TypeError:\n    print(\"operacja niezdefiniowana\")\n\n\na=[1, 2, 3], b=[4, 5, 6]\na*b\noperacja niezdefiniowana\n\n\nBiblioteka Numpy:\n\nimport numpy as np\naa = np.array([1,2,3])\nbb = np.array([4,5,6])\n\nprint(type(aa))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\nCode\nprint(f\"aa={aa}, bb={bb}\")\nprint(f\"aa+bb={aa+bb}\")\nprint(f\"aa*bb={aa*bb}\")\n\n\naa=[1 2 3], bb=[4 5 6]\naa+bb=[5 7 9]\naa*bb=[ 4 10 18]\n\n\nDzięki tak otrzymanym tabelom cech możemy stosować algorytmy tj. XGBoost lub regresji logistycznej w celu wyznaczenia odpowiedniej kombinacji zmiennych wpływających na prawdopodobieństwo dobrego albo i złego klienta.\nPodstawowe systemy bazodanowe związane z językiem SQL również realizują modele danych, w których dane ładnowane są do (ustrukturyzowanych) tabel. \nDane nieustrukturyzowane to takie, które nie są ułożone wtabelarycznej postaci.\n\n!Uwaga - nie oznacza to, iż dane nie możemy przetworzyć do jakiejś postaci tabelarzycznej.\n\nPrzykładem może być dźwięk, obrazczy tekst. Poszczególne litery, częstotliwości czy piksele nie niosą ze sobą żadnych informacji. Nie tworzą osobnych cech, co jest kluczowe dla odróżnienia ich od danych ustrukturyzowanych.\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych (K2A_W02, K2A_W04, O2_W04, O2_W07)",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Od plików płaskich do Data Mash"
    ]
  },
  {
    "objectID": "wyklad1.html#dane",
    "href": "wyklad1.html#dane",
    "title": "Od plików płaskich do Data Mash",
    "section": "",
    "text": "Rozwój technologii informatycznych spowodował dostęp do niewyobrażalnych ilości nowego zasobu jakim są ustrukturyzowane jak i nieustrukturyzowane dane.\nDane przyczyniły się do powstania tysięcy nowych narzędzi do generowania, zbierania, przechowywania i przetwarzania informacji na niespotykaną dotąd skalę.\nZasób ten nie jest nowością i dostępny jest od bardzo dawna. Jednak dopiero po wprowadzeniu systemu pisma można było zacząć prowadzić zapis i przetwarzanie w postaci rachunkowości czy rejestrów różnych rzeczy takich jak: zaludnienie w krajach, spisy rzek, jezior, najgłębsze miejsca itp.\nPojawienie się nowych wyzwań naukowych i biznesowych staje się możliwe do realizacji dzięki budowie systemów opartych na otwartym oprogramowaniu, jak również dzięki wykorzystaniu klastrów komputerów do wspomagania przetwarzania ogromnych ilości danych.\nDziś systemy takie jak SAS, Apache Hadoop,Apache Kafka , Apache Spark, Apache Flink i ich chmurowe odpowiedniki, używane są na szeroką skalę w wielu instytucjach i firmach niemal w każdej dziedzinie. Narzędzia te wykorzystywane są w bankowości, opiece zdrowotnej, naukach przyrodniczych, produkcji, sektorze publicznym czy sprzedaży.\nEpoka danych stawia przed nami coraz to nowsze wyzwania związane nie tylko z ilością, ale i z czasem przetwarzania danych.\nNowe wyzwania biznesowe to między innymi:\n\ninteligentna reklama tysięcy produktów dla milionów klientów,\nprzetwarzanie danych o genach, RNA czy też białkach genus,\ninteligentne wykrywanie różnorodnych sposobów nadużyć wśród setek miliardów transakcji kart kredytowych,\nsymulacje giełdowe oparte o tysiące instrumentów finansowych\n…\n\n\n\nDane ustrukturyzowane zorganizowane są w kolumnach cech charakteryzujących każdą obserwację (wiersze). Kolumny posiadają etykietę, która wskazuje na ich interpretację.\nPrzykładem kolumn mogą być takie cechy jak: płeć, wzrost czy ilość kedytów, na podstawie których można przewidywać czy klient będzie spłacał kredyt czy też nie.\n\n\nCode\ndane_klientow = {\"sex\":[\"m\",\"f\",\"m\",\"m\",\"f\"],\n \"height\":[160, 172, 158, 174, 192],\n \"credits\":[0,0,1,3,1]\n }\n\ndf = pd.DataFrame(dane_klientow)\nprint(df)\n\n\n  sex  height  credits\n0   m     160        0\n1   f     172        0\n2   m     158        1\n3   m     174        3\n4   f     192        1\n\n\nTakie przewidywanie również oznaczane jest jako cecha (ang. target).\n\n\nCode\ndf['target'] = [0,1,1,0,0]\nprint(df)\n\n\n  sex  height  credits  target\n0   m     160        0       0\n1   f     172        0       1\n2   m     158        1       1\n3   m     174        3       0\n4   f     192        1       0\n\n\n\n\n\nW wielu językach programowania domyślnym pojemnikiem na przechowywanie wartości są zmienne.\nwiek = 47\nstan_cywilny = 'kawaler'\nNie są one jednak praktyczną formą do przechowywania i manipulowania danymi.\nJednym z rozwiązań jest przechowywanie wszystkich cech (np. klienta) w jednym obiekcie.\nW Pythonie zadanie to moze realizować obiekt listy, który pozwala przechowywać rózne typy danych w jednym obiekcie.\n\nklient = [38, 'kawaler', 1, 56.3]\nprint(f\"dane klienta {klient} w obiekcie: {type(klient)}\")\n\ndane klienta [38, 'kawaler', 1, 56.3] w obiekcie: &lt;class 'list'&gt;\n\n\nZ punktu widzenia przerwarzania i modelowania mozliwość ta wprowadza więcej problemów niz korzyści. Sprawdźmy domyślne operacje:\n\n\nCode\na = [1,2,3]\nb = [4,5,6]\nprint(f\"a={a}, b={b}\")\nprint(f\"a+b={a+b}\")\n\n\na=[1, 2, 3], b=[4, 5, 6]\na+b=[1, 2, 3, 4, 5, 6]\n\n\nnatomiast:\n\n\nCode\nprint(f\"a={a}, b={b}\")\nprint(f\"a*b\")\ntry:\n    print(f\"a*b= {a*b}\")\nexcept TypeError:\n    print(\"operacja niezdefiniowana\")\n\n\na=[1, 2, 3], b=[4, 5, 6]\na*b\noperacja niezdefiniowana\n\n\nBiblioteka Numpy:\n\nimport numpy as np\naa = np.array([1,2,3])\nbb = np.array([4,5,6])\n\nprint(type(aa))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\nCode\nprint(f\"aa={aa}, bb={bb}\")\nprint(f\"aa+bb={aa+bb}\")\nprint(f\"aa*bb={aa*bb}\")\n\n\naa=[1 2 3], bb=[4 5 6]\naa+bb=[5 7 9]\naa*bb=[ 4 10 18]\n\n\nDzięki tak otrzymanym tabelom cech możemy stosować algorytmy tj. XGBoost lub regresji logistycznej w celu wyznaczenia odpowiedniej kombinacji zmiennych wpływających na prawdopodobieństwo dobrego albo i złego klienta.\nPodstawowe systemy bazodanowe związane z językiem SQL również realizują modele danych, w których dane ładnowane są do (ustrukturyzowanych) tabel. \nDane nieustrukturyzowane to takie, które nie są ułożone wtabelarycznej postaci.\n\n!Uwaga - nie oznacza to, iż dane nie możemy przetworzyć do jakiejś postaci tabelarzycznej.\n\nPrzykładem może być dźwięk, obrazczy tekst. Poszczególne litery, częstotliwości czy piksele nie niosą ze sobą żadnych informacji. Nie tworzą osobnych cech, co jest kluczowe dla odróżnienia ich od danych ustrukturyzowanych.\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych (K2A_W02, K2A_W04, O2_W04, O2_W07)",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Od plików płaskich do Data Mash"
    ]
  },
  {
    "objectID": "wyklad1.html#źródła-danych",
    "href": "wyklad1.html#źródła-danych",
    "title": "Od plików płaskich do Data Mash",
    "section": "Źródła danych",
    "text": "Źródła danych\nDo trzech największych “generatorów” danych należą:\n\ndane społeczne w formie tekstów (tweety, wpisy w portalach społecznościowych, komentarze), zdjęć czy plików wideo. Przydatne do problemów biznesowych realizujących ocenę zachowań i nastrojów konsumentów w analizach marketingowych.\nIoT: dane pochodzące z czujników, czy też logi działania urządzeń i użytkowników (np. na stronie www).\ndane transakcyjne: czyli ogólnie to co w każdej chwili generowane jest jako transakcje pojawiające się zarówno w trybie online jak i w trybie offline.\n\n\nPythonowe źródła danych\n\n\nRzeczywisty proces generowania danych\nDane generowane są w postaci nieograniczonej - pojawiają się na skutek ciągłych działań systemów. W swoim telefonie wygenerowałeś dziś (a nawet na tych zajęciach!) wiele danych. Czy na następnych zajęciach lub tez jutro nie będziesz ich generował?\nDane zawsze generowane są jako jakaś forma strumienia danych.\nSystemy obsługujące strumienie danych: - hurtownie danych - systemy monitorujące działania urządzeń (IoT) - systemy transakcyjne - systemy analityczne stron www - reklamy on-line - media społecznościowe - systemy logowania - ….\n\nfirma to organizacja, która generuje i odpowiada na ciągły strumień danych. Zobacz\n\nW przetwarzaniu wsadowym źródłem (ale i wynikiem przetwarzania) danych jest plik. Jest on zapisywany raz i można się do niego odwołać (może na nim działać wiele procesów - zadań). Nazwa pliku to element identyfikujący zbiór rekordów.\nW przypadku strumienia zdarzenie jest generowane tylko raz przez tzw. producenta (zwanego też nadawcą lub dostawcą). Powstałe zdarzenie przetwarzane może być przez wielu tzw. konsumentów (odbiorców). Zdarzenia strumieniowe grupowane są w tzw. tematy (ang. topics).",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Od plików płaskich do Data Mash"
    ]
  },
  {
    "objectID": "wyklad1.html#big-data",
    "href": "wyklad1.html#big-data",
    "title": "Od plików płaskich do Data Mash",
    "section": "Big Data",
    "text": "Big Data\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.’’ — Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, … four V\n\nVolume (Objętość) - rozmiar danych produkowanych na całym świecie przyrasta w tempie wykładniczym.\nVelocity (Szybkość) - tempo produkowania danych, szybkości ich przesyłania i przetwarzania.\nVariety (Zróżnicowanie) - tradycyjne dane kojarzą się nam z postacią alfanumeryczną złożoną z liter i cyfr. Obecnie mamy do dyspozycji obrazy, dźwięki, pliki wideo, strumienie danych z IoT\nVeracity (Wiarygodność) - Czy dane są kompletne i poprawne? Czy obiektywnie odzwierciedlają rzeczywistość? Czy są podstawą do podejmowania decyzji?\nValue - The value that the data actually holds. In the end, it’s all about cost and benefits.\n\n\nCelem obliczeń nie są liczby, lecz ich zrozumienie R.W. Hamming 1962.",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Od plików płaskich do Data Mash"
    ]
  },
  {
    "objectID": "wyklad1.html#modele-przetwarzania-danych",
    "href": "wyklad1.html#modele-przetwarzania-danych",
    "title": "Od plików płaskich do Data Mash",
    "section": "Modele przetwarzania danych",
    "text": "Modele przetwarzania danych\nDane w biznesie przetwarzane są praktycznie od zawsze. W ciągu ostatnich dziesięcioleci ilość przetwarzanych danych systematycznie rośnie co wpływa na proces przygotowania i przetwarzania danych.\n\nTrochę historii\n\nLata 60-te : Kolekcje danych, bazy danych\nLata 70-te : Relacyjne modele danych i ich implementacja w systemach OLTP\n1975 : Pierwsze komputery osobiste\nLata 80-te : Zaawansowane modele danych, extended-relational, objective oriented, aplikacyjno-zorientowane itp.\n1983 : Początek internetu\nLata 90-te : Data mining, hurtownie danych, systemy OLAP\nPóźniej : NoSQL, Hadoop, SPARK, data lake\n2002 : AWS , 2005: Hadoop, Cloud computing\n\nWiększość danych przechowywana jest w bazach lub hurtowniach danych. Standardowo dostęp do danych sprowadza się najczęściej do realizacji zapytań poprzez aplikację.\nSposób wykorzystania i realizacji procesu dostępu do bazy danych nazywamy modelem przetwarzania. Najczęściej używane są dwie implementacje:\n\n\nModel Tradycyjny\nModel tradycyjny - przetwarzanie transakcyjne w trybie on-line, OLTP (on-line transaction processing). Świetnie sprawdza się w przypadku obsługi bieżącej np. obsługa klienta, rejestr zamówień, obsługa sprzedaży itp. Wykorzystywany w systemach Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nModel ten dostarcza efektywnych rozwiązań m.in do:\n\nefektywnego i bezpiecznego przechowywania danych,\ntransakcyjnego odtwarzanie danych po awarii,\noptymalizacji dostępu do danych,\nzarządzania współbieżnością,\nprzetwarzania zdarzeń -&gt; odczyt -&gt; zapis\n\nCo w przypadku gdy mamy do czynienia z:\n\nagregacjami danych z wielu systemów (np. dla wielu sklepów),\nraportowanie i podsumowania danych,\noptymalizacja złożonych zapytań,\nwspomaganie decyzji biznesowych.\n\nBadania nad tego typu zagadnieniami doprowadziły do sformułowania nowego modelu przetwarzania danych oraz nowego typu baz danych - Hurtownie Danych (Data warehouse).\n\n\nModel OLAP\nPrzetwarzanie analityczne on-line OLAP (on-line analytic processing).\nWspieranie procesów analizy i dostarczanie narzędzi umożliwiających analizę wielowymiarową (czas, miejsce, produkt).\nProces zrzucania danych z różnych systemów do jednej bazy nazywamy Extract-Transform-Load (ETL) (normalizacja i encoding and schema transaction).\nAnaliza danych z hurtowni to przede wszystkim obliczanie agregatów (podsumowań) dotyczących wymiarów hurtowni. Proces ten jest całkowicie sterowany przez użytkownika.\nPrzykład\nZałóżmy, że mamy dostęp do hurtowni danych gdzie przechowywane są informacje dotyczące sprzedaży produktów w supermarkecie. Jak przeanalizować zapytania:\n\nJaka jest łączna sprzedaż produktów w kolejnych kwartałach, miesiącach, tygodniach ?\nJaka jest sprzedaż produktów z podziałem na rodzaje produktów ?\nJaka jest sprzedaż produktów z podziałem na oddziały supermarketu ?\n\nOdpowiedzi na te pytania pozwalają określić wąskie gardła sprzedaży produktów przynoszących deficyt, zaplanować zapasy w magazynach czy porównać sprzedaż różnych grup w różnych oddziałach supermarketu.\nW ramach Hurtowni Danych najczęściej wykonuje się dwa rodzaje zapytań(oba w trybie batchowym): 1. Wykonywane okresowo w czasie zapytania raportowe obliczające biznesowe statystyki 2. Wykonywane ad-hoc zapytania wspomagające krytyczne decyzje biznesowe.\n\n\nWiedza:\n\nZna historię i filozofię modeli przetwarzania danych.\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych\n\n\n\nUmiejętności:\n\nRozróżnia typy danych strukturyzowanych jak i niestrukturyzowanych\n\n\n\nKompetencje:\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Od plików płaskich do Data Mash"
    ]
  },
  {
    "objectID": "wyklad2.html",
    "href": "wyklad2.html",
    "title": "Analiza strumieni danych",
    "section": "",
    "text": "Oczekiwania vs Rzeczywistość\n\nKiedy podjąć decyzję biznesową ?\n\n\n\n\nBatch = Duże, historyczne zbiory\nStream = Strumień danych, on line, przesyłane w trybie ciągłym\n\n\n\n\n\nBatch = minuty, godziny, dni (patrz Hurtownie danych)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = możliwe i stosowane bardzo czesto\nStream = ,,niemożliwe’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)\n\n\n\nSystemy Big data mogą być częścią (źródłem) dla hurtowni danych (np. Data Lake, Enterprise Data Hub)\nAle Hurtownie danych nie są systemami Big Data!\n\nHurtownie danych\n\n\nprzetrzymywanie danych wysoko strukturyzowanych\nskupione na analizach i procesie raportowania\n100% accuracy\n\n\nBig Data\n\n\ndane o dowolnej strukturze\nsłuży do różnorodnych celów opartych na danych (analityka, data science …)\nponiżej 100% accuracy\n\n\n\n\n\nGdy mówimy o skali (nie o języku Scala), najczęściej przychodzi nam na myśl przeglądarka Google. Przeszukuje ona ogromne zbiory danychz dużą prędkością. Sama nazwa Goolge wskasuje na skalę (colowo przyjęto błędną nazwę w zapisie googol co oznacza 1 i 100 zer).\n\nSprawdź czy do końca zajęć uda Ci się zapisać liczbę googol na kartce.\n\nPowinno być dla Ciebie jasne, że żadne tradycyjne systemy, np relacyjne systemy baz danych, ani programowanie imperatywne nie są w stanie obłużyć przeszukiwania takiej ilości danych. Problemy te doprowadziły do budowy rozproszonych systemów plików Google File System, MapReduce (paradygmat programowania równoległego), czy Bigtable (skalowalna pamięć masowa ustrukturyzowanych danych znajdujących się na GFS).\n\n\nZnajdź prosty algorytm map reduce w dowolnym języku programowania i uruchom go.\n\n\nJak poprawić ?",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Analiza strumieni danych"
    ]
  },
  {
    "objectID": "wyklad2.html#batch-vs-stream-processing",
    "href": "wyklad2.html#batch-vs-stream-processing",
    "title": "Analiza strumieni danych",
    "section": "",
    "text": "Oczekiwania vs Rzeczywistość\n\nKiedy podjąć decyzję biznesową ?\n\n\n\n\nBatch = Duże, historyczne zbiory\nStream = Strumień danych, on line, przesyłane w trybie ciągłym\n\n\n\n\n\nBatch = minuty, godziny, dni (patrz Hurtownie danych)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = możliwe i stosowane bardzo czesto\nStream = ,,niemożliwe’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)\n\n\n\nSystemy Big data mogą być częścią (źródłem) dla hurtowni danych (np. Data Lake, Enterprise Data Hub)\nAle Hurtownie danych nie są systemami Big Data!\n\nHurtownie danych\n\n\nprzetrzymywanie danych wysoko strukturyzowanych\nskupione na analizach i procesie raportowania\n100% accuracy\n\n\nBig Data\n\n\ndane o dowolnej strukturze\nsłuży do różnorodnych celów opartych na danych (analityka, data science …)\nponiżej 100% accuracy\n\n\n\n\n\nGdy mówimy o skali (nie o języku Scala), najczęściej przychodzi nam na myśl przeglądarka Google. Przeszukuje ona ogromne zbiory danychz dużą prędkością. Sama nazwa Goolge wskasuje na skalę (colowo przyjęto błędną nazwę w zapisie googol co oznacza 1 i 100 zer).\n\nSprawdź czy do końca zajęć uda Ci się zapisać liczbę googol na kartce.\n\nPowinno być dla Ciebie jasne, że żadne tradycyjne systemy, np relacyjne systemy baz danych, ani programowanie imperatywne nie są w stanie obłużyć przeszukiwania takiej ilości danych. Problemy te doprowadziły do budowy rozproszonych systemów plików Google File System, MapReduce (paradygmat programowania równoległego), czy Bigtable (skalowalna pamięć masowa ustrukturyzowanych danych znajdujących się na GFS).\n\n\nZnajdź prosty algorytm map reduce w dowolnym języku programowania i uruchom go.\n\n\nJak poprawić ?",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Analiza strumieni danych"
    ]
  },
  {
    "objectID": "wyklad2.html#strumienie-danych",
    "href": "wyklad2.html#strumienie-danych",
    "title": "Analiza strumieni danych",
    "section": "Strumienie danych",
    "text": "Strumienie danych\nStrumieniowanie możesz kojarzyć z serwisów przesyłających video w trybie online. Gdy oglądasz swój ulubiony serial (tak jak teraz na zajęciach) serwis odpowiadający za strumieniowanie w nieprzerwany sposób przesyła do ciebie kolejne “porcje” video. Identycznie koncepcja ta realizowana jest w przypadku danych strumieniowych. Format przesyłanych porcji nie musi być plikiem video, wszystko zależy od celu realizowanego biznesowo. Np. ciągły pomiar z różnego rodzaju czujników w farbykach, elektrowniach itp. Warto odnotować, że masz do czynienia z ciągłym strumieniem danych, które przetwarzać musisz w czasie rzeczywistym. Nie możesz czekać do zatrzymania linii produkcyjnych w celu wykonania analizy, wszystkie pojawiające się problemy chcesz rejestrować natychmiast i jak najszybciej na nie reagować.\n\nAnaliza strumieni danych to ciągłe przetwarzanie i analiza dużych zbiorów danych w ruchu.\n\nPorównuj to do wsakazanych powyżej elementów Big Data. Przetwarzanie Batchowe jest przeciwieństwem do przetwarzania strumieniowego. Najpierw zbierasz duże ilości danych a potem realizujesz analizy. Możesz oczywiście zawsze pobrać video w całości zanim je obejrzysz, ale czy miałoby to sens? Istnieją przypadki gdy takie podejście nie stanowi problemu, ale już tu widzisz, że przetwarzanie strumieniowe może przynieść dla biznesu dodatkowe wartości dodane, których trudno oczekiwać przy wsadowym przetwarzaniu.\nciekawe informacje\n\nAnaliza danych w czasie rzeczywistym a przetwarzanie strumienia zdarzeń\nŁatwo jest połączyć analizę w czasie rzeczywistym i analizę strumieniową (lub przetwarzanie strumienia zdarzeń). Ale chociaż technologie analizy strumieniowej mogą umożliwiać analizę w czasie rzeczywistym, to nie to samo!\nAnaliza strumieniowa polega na przetwarzaniu danych w ruchu. Analityka w czasie rzeczywistym to dowolna metoda przetwarzania danych, która skutkuje okresem opóźnienia określanym jako „w czasie rzeczywistym”.\nZazwyczaj systemy analizy czasu rzeczywistego są definiowane jako twarde i miękkie systemy czasu rzeczywistego. Niedotrzymanie terminu w twardych systemach czasu rzeczywistego, takich jak samolot, jest katastrofalne, a w miękkich systemach czasu rzeczywistego, takich jak stacja pogodowa, niedotrzymanie terminów może prowadzić do bezużytecznych danych.\nPonadto, podczas gdy analiza strumieniowa implikuje istnienie architektury strumieniowej, analiza w czasie rzeczywistym nie implikuje żadnej konkretnej architektury.\nWszystko, co implikuje analityka w czasie rzeczywistym, polega na tym, że tworzenie i przetwarzanie danych odbywa się w dowolnym czasie, który firma definiuje jako „w czasie rzeczywistym”.\n\n\nŹródła danych przesyłanych strumieniowo obejmują:\n\nczujniki sprzętu,\nstrumienie kliknięć,\nśledzenie lokalizacji\ninterackcja z użytkownikiem: co robią użytkownicy Twojej witryny?\nkanały mediów społecznościowych,\nnotowania giełdowe,\naktywność w aplikacjach\ninne.\n\nFirmy wykorzystują analitykę strumieniową do odkrywania i interpretowania wzorców, tworzenia wizualizacji, przekazywania spostrzeżeń i alertów oraz uruchamiania procesów w czasie rzeczywistym lub zbliżonym do rzeczywistego.",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Analiza strumieni danych"
    ]
  },
  {
    "objectID": "wyklad2.html#uzasadnienie-biznesowe",
    "href": "wyklad2.html#uzasadnienie-biznesowe",
    "title": "Analiza strumieni danych",
    "section": "Uzasadnienie biznesowe",
    "text": "Uzasadnienie biznesowe\nAnalityka służy do znajdowania znaczących wzorców w danych i odkrywania nowej wiedzy. Dotyczy to zarówno transmisji strumieniowych, jak i tradycyjnych analiz.\nAle w dzisiejszym świecie natura „znajdowania sensownych wzorców w danych” uległa zmianie, ponieważ zmienił się charakter danych. Szybkość, objętość i rodzaje danych eksplodowały.\nTwitter produkuje ponad 500 milionów tweetów dziennie. IDC przewiduje, że do 2025 roku urządzenia Internetu rzeczy (IoT) będą w stanie wygenerować 79,4 zettabajtów (ZB) danych. I te trendy nie wykazują oznak spowolnienia.\nBiorąc pod uwagę nowy charakter danych, główną zaletą analizy strumieniowej jest to, że pomaga ona firmom znajdować znaczące wzorce w danych i odkrywać nową wiedzę ,,w czasie rzeczywistym” lub zbliżonym do rzeczywistego.\n\nktóry pojazd firmowej floty ma prawie pusty bak i gdzie wysłać prowadzącego pojazd do tankowania.\nKtóry pojazd floty zużywa najwięcej paliwa i dlaczego?\nKtóre urządzenia w zakładzie czy fabryce mogą ulec awarii w ciągu najbliższych dni?\nJakie części zamienne trzeba będzie wymienić i w których maszynach w najbliższym czasie ?\nIlu klientów aktualnie robi zakupy w sklepie i czy można im coś zaproponować ?\nCzy klient dzwoni w celu zerwania umowy ?\ni wiele wiele innych.\n\n\nPrzykładowe biznesowe zastosowania\n\nDane z sensorów IoT i detekcja anomalii\nStock Trading (problemy regresyjne) - czas reagowania na zmiany i czas zakupy i sprzedaży akcji.\nClickstream for websites (problem klasyfikacji) - śledzenie i analiza gości na stronie serwisu internetowego - personalizacja strony i treści.\n\n8 najlepszych przykładów analizy w czasie rzeczywistym\nBiznesowe zastosowania",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Analiza strumieni danych"
    ]
  },
  {
    "objectID": "wyklad2.html#definicje",
    "href": "wyklad2.html#definicje",
    "title": "Analiza strumieni danych",
    "section": "Definicje",
    "text": "Definicje\nZapoznaj się z tematem danych strumieniowych\n\nDefinicja 1 - Zdarzenie czyli wszystko co możemy zaobserwować w pewnej chwili czasu. Generowane są jako bezpośredni skutek działania.\nDefinicja 2 - W przypadku danych zdarzenie rozumiemy jako niezmienialny rekord w strumieniu danych zakodowany jako JSON, XML, CSV lub binarnie.\nDefinicja 3 - Ciągły strumień zdarzeń to nieskończony zbiór pojedynczych zdarzeń uporządkowanych w czasie np. logi z urządzenia.\nDefinicja 4 - Strumień danych to dane tworzone przyrostowo w czasie, generowane ze statycznych danych (baza danych, czytanie lini z pliku) bądź w sposób dynamiczny (logi, sensory, funkcje).\n\n\nPrzedsiębiorstwo to organizacja, która generuje i odpowiada na ciągły strumień zdarzeń.\n\nAnalityka strumieniowa (ang. stream analytics) nazywana jest również przetwarzaniem strumieniowym zdarzeń (ang. event stream processing) - przetwarzanie dużej ilości danych już na etapie ich generowania.\nNiezależnie od zastosowanej technologi wszystkie dane powstają jako ciągły strumień zdarzeń (działania użytkowników na stronie www, logi systemowe, pomiary z sensorów).",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Analiza strumieni danych"
    ]
  },
  {
    "objectID": "wyklad2.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "href": "wyklad2.html#czas-w-analizie-danych-w-czasie-rzeczywistym",
    "title": "Analiza strumieni danych",
    "section": "Czas w analizie danych w czasie rzeczywistym",
    "text": "Czas w analizie danych w czasie rzeczywistym\nW przypadku przetwarzania wsadowego przetwarzamy dane historyczne i czas uruchomienia procesu przetwarzania nie ma nic wspólnego z czasem występowania analizowanych zdarzeń.\nDla danych strumieniowych mamy dwie koncepcje czasu:\n\nczas zdarzenia (event time) - czas w którym zdarzenie się wydarzyło.\nczas przetwarzania (processing time) - czas w którym system przetwarza zdarzenie.\n\nW przypadku idealnej sytuacji:\n\nW rzeczywistości przetwarzanie danych zawsze odbywa się z pewnym opóźnieniem, co reprezentowane jest przez punkty pojawiające się poniżej funkcji dla sytuacji idealnej (poniżej diagonalnej).\n\nW aplikacjach przetwarzania strumieniowego istotne okazują się różnice miedzy czasem powstania zdarzenia i jego procesowania. Do najczęstszych przyczyn opóźnienia wyszczególnia się przesyłanie danych przez sieć czy brak komunikacji między urządzeniem a siecią. Prostym przykładem jest tu przejazd samochodem przez tunel i śledzenie położenia przez aplikację GPS.\nMożesz oczywiście zliczać ilość takich pominiętych zdarzeń i uruchomić alarm w sytuacji gdy takich odrzutów będzie za dużo. Drugim (chyba częściej) wykorzystywanym sposobem jest zastosowanie korekty z wykorzystaniem tzw. watermarkingu.\nProces przetwarzania zdarzeń w czasie rzeczywistym można przedstawić w postaci funkcji schodkowej, reprezentowanej na rysunku: \nJak można zauważyć nie wszystkie zdarzenia wnoszą wkład do analizy i przetwarzania. Realizację procesu przetwarzania wraz z uwzględnieniem dodatkowego czasu na pojawienie się zdarzeń (watermark) można przedstawić jako proces obejmujący wszystkie zdarzenia powyżej przerywanej linii. Dodatkowy czas pozwolił na przetworzenie dodatkowych zdarzeń, natomiast nadal mogą zdarzyć się punkty, które nie będą brane pod uwagę.  \nPrzedstawione na wykresach sytuacje jawnie wskazują dlaczego pojęcie czasu jest istotnym czynnikiem i wymaga ścisłego określenia już na poziomie definiowania potrzeb biznesowych. Przypisywanie znaczników czasu do danych (zdarzeń) to trudne zadanie.",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Analiza strumieni danych"
    ]
  },
  {
    "objectID": "wyklad2.html#okna-czasowe",
    "href": "wyklad2.html#okna-czasowe",
    "title": "Analiza strumieni danych",
    "section": "okna czasowe",
    "text": "okna czasowe\nOkno rozłączne (ang. tumbling window) czyli okno o stałej długości. Jego cechą charakterystyczną jest to, iż każde zdarzenie należy tylko do jednego okna.  \nOkno przesuwne (ang. sliding window) obejmuje wszystkie zdarzenia następujące w określonej długości między sobą.  \nOkno skokowe (ang. hopping window) tak jak okno rozłączne ma stałą długość, ale pozwala się w nim na zachodzenie jednych okien na inne. Stosowane zazwyczaj do wygładzenia danych.",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Analiza strumieni danych"
    ]
  },
  {
    "objectID": "wyklad2.html#aplikacje-dla-strumieniowania-danych",
    "href": "wyklad2.html#aplikacje-dla-strumieniowania-danych",
    "title": "Analiza strumieni danych",
    "section": "Aplikacje dla strumieniowania danych",
    "text": "Aplikacje dla strumieniowania danych\nAplikacja przetwarzająca strumień zdarzeń powinna umożliwiać przetworzenie i zapisanie zdarzenia oraz dostęp (w tym samym czasie) do innych danych tak by móc dane zdarzenie przetworzyć (wykonać na nim dowolne przeliczenie) i zapisać jako stan lokalny. Stan ten może być zapisywany w wielu miejscach np. zmienne w programie, pliki lokalne, wew i zew bazy danych. Jedną z najbardziej znanych aplikacji tego typu jest Apache Kafka, którą można łączyć np. z Apache Spark bądź Apache Flink.\nPorównanie z aplikacją w trybie batch\n\nWiedza:\n\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie\n\n\n\nUmiejętności:\n\nRozumie ograniczenia wynikające z czasu przetwarzania przez urządzenia oraz systemy informatyczne\n\n\n\nKompetencje:\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.\nFormułuje problem analityczny wraz z jego informatycznym rozwiązaniem",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Analiza strumieni danych"
    ]
  },
  {
    "objectID": "wyklad1S.html",
    "href": "wyklad1S.html",
    "title": "Wykład 1",
    "section": "",
    "text": "Rozwój technologii informatycznych spowodował dostęp do niewyobrażalnych ilości nowego zasobu jakim są ustrukturyzowane jak i nieustrukturyzowane dane.\n\nDane przyczyniły się do powstania tysięcy nowych narzędzi do generowania, zbierania, przechowywania i przetwarzania informacji na niespotykaną dotąd skalę.\nZasób ten nie jest nowością i dostępny jest od bardzo dawna. Jednak dopiero po wprowadzeniu systemu pisma można było zacząć prowadzić zapis i przetwarzanie w postaci rachunkowości czy rejestrów różnych rzeczy takich jak: zaludnienie w krajach, spisy rzek, jezior, najgłębsze miejsca itp.\nPojawienie się nowych wyzwań naukowych czy biznesowych staje się możliwe do realizacji dzięki budowie systemów opartych na otwartym oprogramowaniu, jak również dzięki wykorzystaniu domowych komputerów do wspomagania przetwarzania ogromnych ilości danych.\nDziś systemy takie jak SAS, Apache Hadoop, Apache Spark, Apache Flink dzięki rozwiązaniom chmurowym używane są na szeroką skalę w wielu instytucjach i firmach niemal w każdej dziedzinie. Narzędzia te wykorzystywane są w bankowości, opiece zdrowotnej, naukach przyrodniczych, produkcji, sektorze publicznym czy sprzedaży.\nEpoka danych stawia przed nami coraz to nowsze wyzwania związane nie tylko z ilością, ale i z czasem przetwarzania danych.\nNowe wyzwania biznesowe to między innymi:\n\ninteligentna reklama tysięcy produktów dla milionów klientów,\nprzetwarzanie danych o genach, RNA czy też białkach genus,\ninteligentne wykrywanie różnorodnych sposobów nadużyć wśród setek miliardów transakcji kart kredytowych,\nsymulacje giełdowe oparte o tysiące instrumentów finansowych\n…\n\nWszystkie algorytmy uczenia maszynowego wymagają danych ustrukturyzowanych zapisanych w tabelarycznej postaci (tensory).\nZorganizowane są one w kolumnach cech charakteryzujących każdą obserwację (wiersze). Przykładem mogą być takie cechy jak: płeć, wzrost czy ilość posiadanych samochodów, na podstawie których można przewidywać czy klient będzie spłacał kredyt czy też nie. Takie przewidywanie również oznaczane jest jako cecha. Zmienne te dobierane są tak, by łatwo można je było pozyskać. Dzięki tak otrzymanym tabelom cech możemy stosować algorytmy tj. XGBoost lub regresji logistycznej w celu wyznaczenia odpowiedniej kombinacji zmiennych wpływających na prawdopodobieństwo dobrego albo i złego klienta.\nPodstawowe systemy bazodanowe związane z językiem SQL również realizują modele danych, w których dane ładnowane są do (ustrukturyzowanych) tabel.\nDane nieustrukturyzowane to takie, które nie są ułożone w~tabelarycznej postaci. &gt; !Uwaga - nie oznacza to, iż dane nie możemy przetworzyć do jakiejś postaci tabelarzycnzej.\nPrzykładem może być dźwięk, obrazczy tekst. Poszczególne litery, częstotliwości~czy piksele nie niosą ze sobą żadnych informacji. Nie tworzą osobnych cech, co jest kluczowe dla odróżnienia ich od danych ustrukturyzowanych.\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych (K2A_W02, K2A_W04, O2_W04, O2_W07)"
  },
  {
    "objectID": "wyklad1S.html#od-plikow-płaskich-do-data-lake",
    "href": "wyklad1S.html#od-plikow-płaskich-do-data-lake",
    "title": "Wykład 1",
    "section": "",
    "text": "Rozwój technologii informatycznych spowodował dostęp do niewyobrażalnych ilości nowego zasobu jakim są ustrukturyzowane jak i nieustrukturyzowane dane.\n\nDane przyczyniły się do powstania tysięcy nowych narzędzi do generowania, zbierania, przechowywania i przetwarzania informacji na niespotykaną dotąd skalę.\nZasób ten nie jest nowością i dostępny jest od bardzo dawna. Jednak dopiero po wprowadzeniu systemu pisma można było zacząć prowadzić zapis i przetwarzanie w postaci rachunkowości czy rejestrów różnych rzeczy takich jak: zaludnienie w krajach, spisy rzek, jezior, najgłębsze miejsca itp.\nPojawienie się nowych wyzwań naukowych czy biznesowych staje się możliwe do realizacji dzięki budowie systemów opartych na otwartym oprogramowaniu, jak również dzięki wykorzystaniu domowych komputerów do wspomagania przetwarzania ogromnych ilości danych.\nDziś systemy takie jak SAS, Apache Hadoop, Apache Spark, Apache Flink dzięki rozwiązaniom chmurowym używane są na szeroką skalę w wielu instytucjach i firmach niemal w każdej dziedzinie. Narzędzia te wykorzystywane są w bankowości, opiece zdrowotnej, naukach przyrodniczych, produkcji, sektorze publicznym czy sprzedaży.\nEpoka danych stawia przed nami coraz to nowsze wyzwania związane nie tylko z ilością, ale i z czasem przetwarzania danych.\nNowe wyzwania biznesowe to między innymi:\n\ninteligentna reklama tysięcy produktów dla milionów klientów,\nprzetwarzanie danych o genach, RNA czy też białkach genus,\ninteligentne wykrywanie różnorodnych sposobów nadużyć wśród setek miliardów transakcji kart kredytowych,\nsymulacje giełdowe oparte o tysiące instrumentów finansowych\n…\n\nWszystkie algorytmy uczenia maszynowego wymagają danych ustrukturyzowanych zapisanych w tabelarycznej postaci (tensory).\nZorganizowane są one w kolumnach cech charakteryzujących każdą obserwację (wiersze). Przykładem mogą być takie cechy jak: płeć, wzrost czy ilość posiadanych samochodów, na podstawie których można przewidywać czy klient będzie spłacał kredyt czy też nie. Takie przewidywanie również oznaczane jest jako cecha. Zmienne te dobierane są tak, by łatwo można je było pozyskać. Dzięki tak otrzymanym tabelom cech możemy stosować algorytmy tj. XGBoost lub regresji logistycznej w celu wyznaczenia odpowiedniej kombinacji zmiennych wpływających na prawdopodobieństwo dobrego albo i złego klienta.\nPodstawowe systemy bazodanowe związane z językiem SQL również realizują modele danych, w których dane ładnowane są do (ustrukturyzowanych) tabel.\nDane nieustrukturyzowane to takie, które nie są ułożone w~tabelarycznej postaci. &gt; !Uwaga - nie oznacza to, iż dane nie możemy przetworzyć do jakiejś postaci tabelarzycnzej.\nPrzykładem może być dźwięk, obrazczy tekst. Poszczególne litery, częstotliwości~czy piksele nie niosą ze sobą żadnych informacji. Nie tworzą osobnych cech, co jest kluczowe dla odróżnienia ich od danych ustrukturyzowanych.\n\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych (K2A_W02, K2A_W04, O2_W04, O2_W07)"
  },
  {
    "objectID": "wyklad1S.html#źródła-danych",
    "href": "wyklad1S.html#źródła-danych",
    "title": "Wykład 1",
    "section": "Źródła danych",
    "text": "Źródła danych\nDo trzech największych “generatorów” danych należą:\n\ndane społeczne w formie tekstów (tweety, wpisy w~portalach społecznościowych, komentarze), zdjęć czy plików wideo. Przydatne do problemów biznesowych realizujących ocenę zachowań i nastrojów konsumentów w analizach marketingowych.\nIoT: dane pochodzące z czujników, czy też logi działania urządzeń i użytkowników (np. na stronie www).\nDane transakcyjne: czyli ogólnie to co w każdej chwili generowane jest jako transakcje pojawiające się zarówno w trybie online jak i w trybie offline.\n\n\nRzeczywisty proces generowania danych\nDane generowane są w postaci nieograniczonej - pojawiają się na skutek ciągłych działań systemów. W swoim telefonie wygenerowałeś dziś (a nawet na tych zajęciach!) wiele danych. Czy na następnych zajęciach lub tez jutro nie będziesz ich generował?\nDane zawsze generowane są jako jakaś forma strumienia danych.\nSystemy obsługujące strumienie danych: - hurtownie danych - systemy monitorujące działania urządzeń (IoT) - systemy transakcyjne - systemy analityczne stron www - reklamy on-line - media społecznościowe - systemy logowania - ….\n\nfirma to organizacja, która generuje i odpowiada na ciągły strumień danych. Zobacz\n\nW przetwarzaniu wsadowym źródłem (ale i wynikiem przetwarzania) danych jest plik. Jest on zapisywany raz i można się do niego odwołać (może na nim działać wiele procesów - zadań). Nazwa pliku to element identyfikujący zbiór rekordów.\nW przypadku strumienia zdarzenie jest generowane tylko raz przez tzw. producenta (zwanego też nadawcą lub dostawcą). Powstałe zdarzenie przetwarzane może być przez wielu tzw. konsumentów (odbiorców). Zdarzenia strumieniowe grupowane są w tzw. tematy (ang. topics)."
  },
  {
    "objectID": "wyklad1S.html#big-data",
    "href": "wyklad1S.html#big-data",
    "title": "Wykład 1",
    "section": "Big Data",
    "text": "Big Data\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.’’ — Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, … four V\n\nVolume (Objętość) - rozmiar danych produkowanych na całym świecie przyrasta w tempie wykładniczym.\nVelocity (Szybkość) - tempo produkowania danych, szybkości ich przesyłania i przetwarzania.\nVariety (Zróżnicowanie) - tradycyjne dane kojarzą się nam z postacią alfanumeryczną złożoną z liter i cyfr. Obecnie mamy do dyspozycji obrazy, dźwięki, pliki wideo, strumienie danych z IoT\nVeracity (Wiarygodność) - Czy dane są kompletne i poprawne? Czy obiektywnie odzwierciedlają rzeczywistość? Czy są podstawą do podejmowania decyzji?\nValue - The value that the data actually holds. In the end, it’s all about cost and benefits.\n\n\nCelem obliczeń nie są liczby, lecz ich zrozumienie R.W. Hamming 1962."
  },
  {
    "objectID": "wyklad1S.html#modele-przetwarzania-danych",
    "href": "wyklad1S.html#modele-przetwarzania-danych",
    "title": "Wykład 1",
    "section": "Modele przetwarzania danych",
    "text": "Modele przetwarzania danych\nDane w biznesie przetwarzane są praktycznie od zawsze. W ciągu ostatnich dziesięcioleci ilość przetwarzanych danych systematycznie rośnie co wpływa na proces przygotowania i przetwarzania danych.\n\nTrochę historii\n\nLata 60-te : Kolekcje danych, bazy danych\nLata 70-te : Relacyjne modele danych i ich implementacja w systemach OLTP\n1975 : Pierwsze komputery osobiste\nLata 80-te : Zaawansowane modele danych, extended-relational, objective oriented, aplikacyjno-zorientowane itp.\n1983 : Początek internetu\nLata 90-te : Data mining, hurtownie danych, systemy OLAP\nPóźniej : NoSQL, Hadoop, SPARK, data lake\n2002 : AWS , 2005: Hadoop, Cloud computing\n\nWiększość danych przechowywana jest w bazach lub hurtowniach danych. Standardowo dostęp do danych sprowadza się najczęściej do realizacji zapytań poprzez aplikację.\nSposób wykorzystania i realizacji procesu dostępu do bazy danych nazywamy modelem przetwarzania. Najczęściej używane są dwie implementacje:\n\n\nModel Tradycyjny\nModel tradycyjny - przetwarzanie transakcyjne w trybie on-line, OLTP (on-line transaction processing). Świetnie sprawdza się w przypadku obsługi bieżącej np. obsługa klienta, rejestr zamówień, obsługa sprzedaży itp. Wykorzystywany w systemach Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications.\n\nModel ten dostarcza efektywnych rozwiązań m.in do:\n\nefektywnego i bezpiecznego przechowywania danych,\ntransakcyjnego odtwarzanie danych po awarii,\noptymalizacji dostępu do danych,\nzarządzania współbieżnością,\nprzetwarzania zdarzeń -&gt; odczyt -&gt; zapis\n\nCo w przypadku gdy mamy do czynienia z:\n\nagregacjami danych z wielu systemów (np. dla wielu sklepów),\nraportowanie i podsumowania danych,\noptymalizacja złożonych zapytań,\nwspomaganie decyzji biznesowych.\n\nBadania nad tego typu zagadnieniami doprowadziły do sformułowania nowego modelu przetwarzania danych oraz nowego typu baz danych - Hurtownie Danych (Data warehouse).\n\n\nModel OLAP\nPrzetwarzanie analityczne on-line OLAP (on-line analytic processing).\nWspieranie procesów analizy i dostarczanie narzędzi umożliwiających analizę wielowymiarową (czas, miejsce, produkt).\nProces zrzucania danych z różnych systemów do jednej bazy nazywamy Extract-Transform-Load (ETL) (normalizacja i encoding and schema transaction).\nAnaliza danych z hurtowni to przede wszystkim obliczanie agregatów (podsumowań) dotyczących wymiarów hurtowni. Proces ten jest całkowicie sterowany przez użytkownika.\nPrzykład\nZałóżmy, że mamy dostęp do hurtowni danych gdzie przechowywane są informacje dotyczące sprzedaży produktów w supermarkecie. Jak przeanalizować zapytania:\n\nJaka jest łączna sprzedaż produktów w kolejnych kwartałach, miesiącach, tygodniach ?\nJaka jest sprzedaż produktów z podziałem na rodzaje produktów ?\nJaka jest sprzedaż produktów z podziałem na oddziały supermarketu ?\n\nOdpowiedzi na te pytania pozwalają określić wąskie gardła sprzedaży produktów przynoszących deficyt, zaplanować zapasy w magazynach czy porównać sprzedaż różnych grup w różnych oddziałach supermarketu.\nW ramach Hurtowni Danych najczęściej wykonuje się dwa rodzaje zapytań(oba w trybie batchowym): 1. Wykonywane okresowo w czasie zapytania raportowe obliczające biznesowe statystyki 2. Wykonywane ad-hoc zapytania wspomagające krytyczne decyzje biznesowe."
  },
  {
    "objectID": "wyklad1S.html#źródła-danych-przesyłanych-strumieniowo-obejmują",
    "href": "wyklad1S.html#źródła-danych-przesyłanych-strumieniowo-obejmują",
    "title": "Wykład 1",
    "section": "Źródła danych przesyłanych strumieniowo obejmują:",
    "text": "Źródła danych przesyłanych strumieniowo obejmują:\n\nczujniki sprzętu,\nstrumienie kliknięć,\nśledzenie lokalizacji\ninterackcja z użytkownikiem: co robią użytkownicy Twojej witryny?\nkanały mediów społecznościowych,\nnotowania giełdowe,\naktywność w aplikacjach\ninne.\n\nFirmy wykorzystują analitykę strumieniową do odkrywania i interpretowania wzorców, tworzenia wizualizacji, przekazywania spostrzeżeń i alertów oraz uruchamiania procesów w czasie rzeczywistym lub zbliżonym do rzeczywistego.\n\nAnaliza danych w czasie rzeczywistym a przetwarzanie strumienia zdarzeń\nŁatwo jest połączyć analizę w czasie rzeczywistym i analizę strumieniową (lub przetwarzanie strumienia zdarzeń). Ale chociaż technologie analizy strumieniowej mogą umożliwiać analizę w czasie rzeczywistym, to nie to samo!\nAnaliza strumieniowa polega na przetwarzaniu danych w ruchu. Analityka w czasie rzeczywistym to dowolna metoda przetwarzania danych, która skutkuje okresem opóźnienia określanym jako „w czasie rzeczywistym”.\nZazwyczaj systemy analizy czasu rzeczywistego są definiowane jako twarde i miękkie systemy czasu rzeczywistego. Niedotrzymanie terminu w twardych systemach czasu rzeczywistego, takich jak samolot, jest katastrofalne, a w miękkich systemach czasu rzeczywistego, takich jak stacja pogodowa, niedotrzymanie terminów może prowadzić do bezużytecznych danych.\nPonadto, podczas gdy analiza strumieniowa implikuje istnienie architektury strumieniowej, analiza w czasie rzeczywistym nie implikuje żadnej konkretnej architektury.\nWszystko, co implikuje analityka w czasie rzeczywistym, polega na tym, że tworzenie i przetwarzanie danych odbywa się w dowolnym czasie, który firma definiuje jako „w czasie rzeczywistym”."
  },
  {
    "objectID": "wyklad1S.html#uzasadnienie-biznesowe",
    "href": "wyklad1S.html#uzasadnienie-biznesowe",
    "title": "Wykład 1",
    "section": "Uzasadnienie biznesowe",
    "text": "Uzasadnienie biznesowe\nAnalityka służy do znajdowania znaczących wzorców w danych i odkrywania nowej wiedzy. Dotyczy to zarówno transmisji strumieniowych, jak i tradycyjnych analiz.\nAle w dzisiejszym świecie natura „znajdowania sensownych wzorców w danych” uległa zmianie, ponieważ zmienił się charakter danych. Szybkość, objętość i rodzaje danych eksplodowały.\nTwitter produkuje ponad 500 milionów tweetów dziennie. IDC przewiduje, że do 2025 roku urządzenia Internetu rzeczy (IoT) będą w stanie wygenerować 79,4 zettabajtów (ZB) danych. I te trendy nie wykazują oznak spowolnienia.\nBiorąc pod uwagę nowy charakter danych, główną zaletą analizy strumieniowej jest to, że pomaga ona firmom znajdować znaczące wzorce w danych i odkrywać nową wiedzę ,,w czasie rzeczywistym” lub zbliżonym do rzeczywistego.\n\nktóry pojazd firmowej floty ma prawie pusty bak i~gdzie wysłać prowadzącego pojazd do tankowania.\nKtóry pojazd floty zużywa najwięcej paliwa i~dlaczego?\nKtóre urządzenia w~zakładzie czy fabryce mogą ulec awarii w~ciągu najbliższych dni?\nJakie części zamienne trzeba będzie wymienić iwktórych maszynach w~najbliższym czasie ?\nIlu klientów aktualnie robi zakupy w~sklepie i~czy można im coś zaproponować ?\nCzy klient dzwoni w~celu zerwania umowy ?\ni wiele wiele innych.\n\n8 najlepszych przykładów\nBiznesowe zastosowania"
  },
  {
    "objectID": "wyklad1S.html#definicje",
    "href": "wyklad1S.html#definicje",
    "title": "Wykład 1",
    "section": "Definicje",
    "text": "Definicje\nZapoznaj się z tematem danych strumieniowych\n\nDefinicja 1 - Zdarzenie czyli wszystko co możemy zaobserwować w pewnej chwili czasu. Definicja 2 - W przypadku danych zdarzenie rozumiemy jako niezmienialny rekord w strumieniu danych zakodowany jako JSON, XML, CSV lub binarnie. Definicja 3 - Ciągły strumień zdarzeń to nieskończony zbiór pojedynczych zdarzeń uporządkowanych w czasie np. logi z urządzenia.\n\n\nPrzedsiębiorstwo to organizacja, która generuje i odpowiada na ciągły strumień zdarzeń.\n\n\nDefinicja 4 - Strumień danych to dane tworzone przyrostowo w czasie, generowane ze statycznych danych (baza danych, czytanie lini z pliku) bądź w sposób dynamiczny (logi, sensory, funkcje).\n\nAnalityka strumieniowa (ang. stream analytics) nazywana jest również przetwarzaniem strumieniowym zdarzen (ang. event stream processing) - przetwarzanie dużej ilości danych już na etapie ich generowania.\nGenerowane są jako bezpośredni skutek działania.\nNiezależnie od zastosowanej technologi wszystkie dane powstają jako ciągły strumień zdarzeń (działania użytkowników na stronie www, logi systemowe, pomiary z sensorów)."
  },
  {
    "objectID": "wyklad1S.html#aplikacje-dla-strumieniowania-danych",
    "href": "wyklad1S.html#aplikacje-dla-strumieniowania-danych",
    "title": "Wykład 1",
    "section": "Aplikacje dla strumieniowania danych",
    "text": "Aplikacje dla strumieniowania danych\nAplikacja przetwarzająca strumień zdarzeń powinna umożliwiać przetworzenie i zapisanie zdarzenia oraz dostęp (w tym samym czasie) do innych danych tak by móc dane zdarzenie przetworzyć (wykonać na nim dowolne przeliczenie) i zapisać jako stan lokalny. Stan ten może być zapisywany w wielu miejscach np. zmienne w programie, pliki lokalne, wew i zew bazy danych. Jedną z najbardziej znanych aplikacji tego typu jest Apache Kafka, którą można łączyć np. z Apache Spark bądź Apache Flink.\nPorównanie z aplikacją w trybie batch"
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup ksiązkę\n\n\n\n\n\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie III. Zobacz opis, Kup książkę lub Kup e-book.\nSebastian Raschka, Vahid Mirjalili Python. Machine learning i deep learning. Biblioteki scikit-learni TensorFlow 2. Wydanie III. Zobacz opis lub Kup książkę.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska Jupyter. Wydanie III Zobacz opis, Kup książkę lub Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. Błyskawiczna analiza danych Zobacz opis lub Kup\nAkash Tandon, Sandy Ryza, Uri Laserson, Sean Owen, Josh Wills, Zaawansowana analiza danych w PySpark Zobacz opis Kup ksiązkę lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#książki",
    "href": "ksiazki.html#książki",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "G. Maas, F. Garillot Stream Processing with Apache Spark Zobacz opis lub Kup e-book\nF. Hueske, V. Kalavri Stream Processing with Apache Flink Zobacz opis lub Kup e-book\nA. Bellemare Mikrousługi oparte na zdarzeniach. Wykorzystanie danych w organizacji na dużą skalę Zobacz opis lub Kup ksiązkę\n\n\n\n\n\nA. Geron Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow. Wydanie III. Zobacz opis, Kup książkę lub Kup e-book.\nSebastian Raschka, Vahid Mirjalili Python. Machine learning i deep learning. Biblioteki scikit-learni TensorFlow 2. Wydanie III. Zobacz opis lub Kup książkę.\nW. McKinney Python w analizie danych. Przetwarzanie danych za pomocą pakietów Pandas i NumPy oraz środowiska Jupyter. Wydanie III Zobacz opis, Kup książkę lub Kup e-book\nD. McIlwraith, H. Marmanis, D. Babenko Inteligentna sieć. Algorytmy przyszłości. Wydanie II (ebook) Zobacz opis lub Kup książkę, Kup e-book\nJoel Grus Data science od podstaw. Analiza danych w Pythonie. Wydanie II. Zobacz opis lub Kup książkę, Kup e-book.\nJohn W. Foreman Mistrz analizy danych. Od danych do wiedzy. Zobacz opis lub Kup książkę, Kup e-book.\nAlberto Boschetti, Luca Massaron Python. Podstawy nauki o danych. Zobacz opis lub Kup książkę.\nR. Schutt, C. O’Neil Badanie danych. Raport z pierwszej lini działań. Zobacz opis lub Kup książkę.\nT. Segaran Nowe usługi 2.0. Przewodnik po analizie zbiorów danych Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nF. Chollet Deep Learning. Praca z językiem Python i biblioteką Keras. Zobacz opis lub Kup książkę, Kup e-book\nJ. Patterson, A. Gibson Deep Learning. Praktyczne wprowadzenie (ebook) Zobacz opis lub Kup e-book\nV. Zocca, G. Spacagna, D. Slater, P. Roelants. Deep Learning. Uczenie głębokie z językiem Python. Sztuczna inteligencja i sieci neuronowe Zobacz opis lub Kup ebook\nD. Osinga Deep Learning. Receptury Zobacz opis lub Kup książkę, Kup e-book\nS. Weidman Uczenie głębokie od zera. Podstawy implementacji w Pythonie Zobacz opis lub Kup książkę, Kup e-book\nD. Foster Deep learning i modelowanie generatywne. Jak nauczyć komputer malowania, pisania, komponowania i grania Zobacz opis lub Kup książkę, Kup e-book\nJ. Howard, S. Gugger Deep learning dla programistów. Budowanie aplikacji AI za pomocą fastai i PyTorch Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nJ. S. Damji, B. Wenig, T. Das, D. Lee Spark. Błyskawiczna analiza danych Zobacz opis lub Kup\nAkash Tandon, Sandy Ryza, Uri Laserson, Sean Owen, Josh Wills, Zaawansowana analiza danych w PySpark Zobacz opis Kup ksiązkę lub Kup e-book\nB. Chambers, M. Zaharia Spark: The Definitive Guide. Big Data Processing Made Simple (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nG. Coldwind Zrozumieć programowanie Zobacz opis lub Kup książkę, Kup e-book\nA. Allain C++. Przewodnik dla początkujących Zobacz opis lub Kup książkę, Kup e-book\nS. Dasgupta, C. Papadimitriou, U. Vazirani Algorytmy PWN.\n\n\n\n\n\nJ. Krochmalski Docker. Projektowanie i wdrażanie aplikacji Zobacz opis lub Kup książkę, Kup e-book\nR. McKendrick, S. Gallagher Docker. Programowanie aplikacji dla zaawansowanych. Wydanie II Zobacz opis lub Kup książkę, Kup e-book\n\n\n\n\n\nP. Bell, B. Beer GitHub. Przyjazny przewodnik (ebook) Zobacz opis lub Kup e-book\n\n\n\n\n\nC. Althoff, Programista Samouk. Profesjonalny przewodnik do samodzielnej nauki kodowania. Zobacz opis lub Kup teraz, Kup e-book\nA. Sweigart, Automatyzacja nudnych zadań z pythonem. Zobacz opis lub Kup książkę, Kup e-book\nK. Reitz, T. Schlusser Przewodnik po Pythonie. Dobre praktyki i praktyczne narzędzia. Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nB.Tate, L. Carslon, C. Hiibs, Ruby on Rails. Wprowadzenie. Wydanie II Zobacz opis lub Kup e-book\nB. Frain, Responsive Web Design. Projektowanie elastycznych witryn w HTML5 i CSS3, Zobacz opis lub Kup e-book\nK. Beck, TDD. Sztuda tworzenia, Zobacz opis lub Kup teraz, Kup e-book\nB. Dayley, Node.js, MongoDB, AngularJS. Kompendium wiedzy, Zobacz opis lub Kup teraz, Kup e-book\n\n\n\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "Książki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nSoftware\n\nGithub\nGit-instrukcja\nwww.python.org\nPyPI python libraries\nAnaconda\nDocker\n\n\n\nPakiety python dla analiz danych\n\nNumPy\nSciPy\nPandas\nScikit-learn\nJupyter\nMatplotlib\nBeautiful Soup\nTheano\nKeras\nTensorFlow\nVirtual ENV\n\n\n\nEdytory tekstu\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nJupyter notebook\n\nGaleria ciekawych notatników\nIntro\nKernels\nBringing the best out of jupyter for data science\nJupyter extensions\nI don’t like notebooks\nJupyter lab\nSpeed up jupyter notebook\n\n\n\nPrzetwarzanie danych\n\ndata cookbook\n\n\n\nZbiory danych\n\nInternet Archive\nReddit\nKDnuggets\nKaggle\nList of datasets for machine learning research\nUCI Machine Learning Repo\nPublic API\nGoogle Datatset Search\n\n\n\nPython\n\nChris Albon Technical Notes on Using Data Science & AI\n40+ Python Statistics For Data Science Resources\nPractical Business Python\n\n\n\nkursy ML\n\nKurs Machine Learning - Andrew Ng, Stanford\nKurs Machine Learning - Andrew Ng, Stanford\nPython programming for data science"
  },
  {
    "objectID": "sylabus.html",
    "href": "sylabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Nazwa przedmiotu: Analiza danych w czasie rzeczywistym\nJednostka: SGH w Warszawie\nKod przedmiotu: 222890-D, 222890-S\nPunkty ECTS: 3\nJęzyk prowadzenia: polski\nPoziom przedmiotu: średnio-zaawansowany\nProwadzący: Sebastian Zając, sebastian.zajac@sgh.waw.pl\nWebsite: https://sebkaz-teaching.github.io/RTA_2023/"
  },
  {
    "objectID": "sylabus.html#cel-przedmiotu",
    "href": "sylabus.html#cel-przedmiotu",
    "title": "Syllabus",
    "section": "Cel Przedmiotu",
    "text": "Cel Przedmiotu\nPodejmowanie prawidłowych decyzji opartych na danych i ich analizie jest niezwykle istotne w dzisiejszym i nowoczesnym biznesie. Wprowadzenie nowoczesnych metod takich jak uczenie maszynowe, sztuczna inteligencja i głębokie sieci neuronowe, może znacznie poprawić zarówno zrozumienie biznesu, jak i jakość podejmowanych decyzji. Ponadto, szybkośc podejmowania decyzji jest kluczowym czynnikiem w dynamicznym środowisku biznesowym, zwłaszcza tam, gdzie pracuje się bezpośrednio z klientem. Zajęcia mają na celu przekazanie studentom doświadczenia oraz kompleksowej wiedzy teoretycznej w zakresie przetwarzania i analizy danych w czasie rzeczywistym oraz zaprezentowanie najnowszych technologii informatycznych służących do przetwarzania danych ustrukturyzowanych (pochodzących np. z hurtowni danych) jak i nieustrukturyzowanych (np. obrazy, dźwięk, strumieniowanie video) w trybie on-line. W toku zajęć przedstawiona zostanie filozofia analizy dużych danych w czasie rzeczywistym jz wykorzystaniem programowania w języku Python. Przedstawione zostaną struktury oprogramowania służące do przetwarzania danych wraz z omówieniem problemów i trudności jakie spotyka się w realizacji modelowania w czasie rzeczywistym dla dużej ilości danych. Wiedza teoretyczna zdobywana będzie (oprócz części wykładowej) poprzez realizację przypadków testowych w narzędziach takich jak Apache Spark czy Apache Kafka. Na zajęciach laboratoryjnych studenci korzystać będą z pełni skonfigurowanych środowisk programistycznych przygotowanych do przetwarzania, modelowania i analizy danych. Tak, aby oprócz umiejętności i znajomości technik analitycznych studenci poznali i zrozumieli najnowsze technologie informatyczne związane z przetwarzaniem danych w czasie rzeczywistym."
  },
  {
    "objectID": "sylabus.html#program-przedmiotu",
    "href": "sylabus.html#program-przedmiotu",
    "title": "Syllabus",
    "section": "Program przedmiotu",
    "text": "Program przedmiotu\n\nOd plików płaskich do Data Mash. Modele przetwarzania danych w Big Data.\nETL i modelowanie w trybie wsadowym (offline learning) i przyrostowym (online learning). Map-Reduce.\nStrumienie danych, zdarzenia i koncepcje czasu i okien czasowych w przetwarzaniu danych w czasie rzeczywistym.\nMikroserwisy i komunikacja przez REST API.\nWspółczesne architektury aplikacji do przetwarzania danych strumieniowych - Lambda, Kappa, Pub/Sub.\nPrzetwarzanie ustrukturyzowanych i niestrukturyzowanych danych. Środowisko programistyczne dla języka Python.\nWykorzystanie obiektowych elementów Pythona w procesie modelowania za pomocą Scikit-Learn i Keras\nPodstawy OOP dla języka Python. Budowa klasy dla algorytmu błądzenia losowego, Perceprtonu i Adeline.\nPrzygotowanie mikroserwisu z modelem ML do zastosowania produkcyjnego.\nStrumieniowanie danych z wykorzystaniem RDD dla Apache Spark. Wprowadzenie do obiektu DataFrame."
  },
  {
    "objectID": "sylabus.html#efekty-kształcenia",
    "href": "sylabus.html#efekty-kształcenia",
    "title": "Syllabus",
    "section": "Efekty kształcenia",
    "text": "Efekty kształcenia\n\nWiedza:\n\n\nZna historię i filozofię modeli przetwarzania danych Powiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W02, (OGL)O2_W04, (OGL)O2_W07 Metody weryfikacji: kolokwium pisemne (pytania otwarte, zadania) Metody dokumentacji: wykaz pytań z kolokwium\nZna typy danych ustrukturyzowanych jak i nieustrukturyzowanych Powiązania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W04, (OGL)O2_W04, (OGL) O2_W07 Metody weryfikacji: projekt Metody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym Powiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W02, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W08 Metody weryfikacji: egzamin pisemny (pytania otwarte, zadania) Metody dokumentacji: wykaz pytań egzaminacyjnych\nZna teoretyczne aspekty struktury lambda i kappa Powiązania: (Analiza danych - Big Data)K2A_W03, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W04, (OGL) O2_W06, (OGL)O2_W08 Metody weryfikacji: kolokwium pisemne (pytania otwarte, zadania) Metody dokumentacji: wykaz pytań z kolokwium\nUmie wybrać strukturę IT dla danego problemu biznesowego Powiązania: (Analiza danych - Big Data)K2A_W02, (Analiza danych - Big Data)K2A_W03, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08 Metody weryfikacji: projekt Metody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie Powiązania: (Analiza danych - Big Data)K2A_W01, (Analiza danych - Big Data)K2A_W05, (OGL)O2_W01, (OGL) O2_W04, (OGL)O2_W06, (OGL)O2_W08 Metody weryfikacji: projekt Metody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\n\nUmiejętności:\n\n\nRozróżnia typy danych strukturyzowanych jak i niestrukturyzowanych Powiązania: K2A_U02, K2A_U07, K2A_U10, O2_U02 Metody weryfikacji: test Metody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\nUmie przygotować, przetwarzać oraz zachowywać dane generowane w czasie rzeczywistym Powiązania: K2A_U03, K2A_U05, K2A_U09, O2_U02, O2_U04 Metody weryfikacji: projekt Metody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\nRozumie ograniczenia wynikające z czasu przetwarzania przez urządzenia oraz systemy informatyczne Powiązania: K2A_U01, K2A_U07, K2A_U11, O2_U02 Metody weryfikacji: projekt Metody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\nUmie zastosować i skonstruować system do przetwarzania w czasie rzeczywistym Powiązania: K2A_U05, K2A_U10, O2_U05, O2_U06, O2_U07 Metody weryfikacji: projekt Metody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\nUmie przygotować raportowanie dla systemu przetwarzania w czasie rzeczywistym Powiązania: K2A_U02, K2A_U08, K2A_U10, O2_U06, O2_U07 Metody weryfikacji: projekt Metody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)\n\n\nKompetencje:\n\n\nFormułuje problem analityczny wraz z jego informatycznym rozwiązaniem Powiązania: K2A_K01, K2A_K03, O2_K02, O2_K06, O2_K07 Metody weryfikacji: projekt, prezentacja Metody dokumentacji: prace pisemne studenta (w trakcie semestru, zaliczeniowe, egzaminacyjne)\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym. Powiązania: K2A_K02, K2A_K04, (OGL)O2_K01, (OGL) O2_K02, (OGL)O2_K05, (OGL)O2_K06 Metody weryfikacji: projekt Metody dokumentacji: prace pisemne studenta ( w trakcie semestru, zaliczeniowe, egzaminacyjne)"
  },
  {
    "objectID": "sylabus.html#realizacja-przedmiotu",
    "href": "sylabus.html#realizacja-przedmiotu",
    "title": "Syllabus",
    "section": "Realizacja przedmiotu",
    "text": "Realizacja przedmiotu\n\negzamin testowy 30%\nkolokwium 30%\nreferaty/eseje 40%\n\n## Literatura\n\nZając S. “Modelowanie dla biznesu. Analityka w czasie rzeczywistym - narzędzia informatyczne i biznesowe. Oficyna Wydawnicza SGH, Warszawa 2022\nK. Przanowski K. , Zając S. red. “Modelowanie dla biznesu, metody ML, modele portfela CF, modele rekurencyjne, analizy przeżycia, modele scoringowe, SGH, Warszawa 2020.\nFrątczak E., red. “Modelowanie dla biznesu, Regresja logistyczna, Regresja Poissona, Survival Data Mining, CRM, Credit Scoring”. SGH, Warszawa 2019.\nS. Raschka, Python. Uczenie maszynowe. Wydanie II\nMaas G., Garillot F. Stream Processing with Apache Spark, O’Reilly, 2021\nF. Hueske, V. Kalavri Stream Processing with Apache Flink, O’Reilly, 2021\nNandi A. “Spark for Python Developers”, 2015"
  },
  {
    "objectID": "sylabus.html#literatura-uzupełniająca",
    "href": "sylabus.html#literatura-uzupełniająca",
    "title": "Syllabus",
    "section": "Literatura uzupełniająca",
    "text": "Literatura uzupełniająca\n\nFrątczak E., “Statistics for Management & Economics” SGH, Warszawa, 2015\nSimon P., “Too Big to IGNORE. The Business Case for Big Data”, John Wiley & Sons Inc., 2013\nNandi A. “Spark for Python Developers”, 2015\nFrank J. Ohlhorst. “Big Data Analytics. Turning Big Data into Big Money”. John Wiley & Sons. Inc. 2013\nRussell J. “Zwinna analiza danych Apache Hadoop dla każdego”, Helion, 2014\nTodman C., “Projektowanie hurtowni danych, Wspomaganie zarządzania relacjami z klientami”, Helion, 2011"
  },
  {
    "objectID": "wyklad3.html",
    "href": "wyklad3.html",
    "title": "Mikroserwisy i komuniakcja przez REST API.",
    "section": "",
    "text": "Komunikacja sieciowa, relacyjne bazy danych, rozwiązania chmurowe i big data znacząco zmieniły sposób budowania systemów informatycznych i wykonywnia na niach pracy.\nPorównaj to jak “narzędzia” do realizacji przekazu (gazeta, radio, telewizja, internet, komunikatory, media społecznościowe) zmieniły interakcje międzyludzkie i struktury społeczne.\nKoncepcja mikrousługi (mikroserwisu) jest bardzo popularnym sposobem budowania systemów informatycznych jak i koncepcją przy tworzeniu oprogramowania czy realizacji firmy w duchu Data-Driven. Koncepcja ta pozwala zachować wydajność (rób jedną rzecz ale dobrze), elastyczność i jasną postać całej struktury.\nChociaż istnieją inne sposoby architektury projektów oprogramowania, „mikroserwisy” są często używane nie bez powodu. Idea mikroserwisów tkwi w nazwie: oprogramowanie jest reprezentowane jako wiele małych usług, które działają indywidualnie. Patrząc na ogólną architekturę, każda mikrousługa znajduje się w małej czarnej skrzynce z jasno zdefiniowanymi wejściami i wyjściami. Możesz porównać tego typu zachowanie do “czystej funkcji” w programowaniu funkcyjnym.\nW celu umożliwienia komunikacji różnych mikroserwisów często wybieranym rozwiązaniem jest wykorzystanie Application Programming Interfaces API .",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Mikroserwisy i komuniakcja przez REST API."
    ]
  },
  {
    "objectID": "wyklad3.html#komunikacja-przez-api",
    "href": "wyklad3.html#komunikacja-przez-api",
    "title": "Mikroserwisy i komuniakcja przez REST API.",
    "section": "Komunikacja przez API",
    "text": "Komunikacja przez API\nCentralnym elementem architektury mikrousług jest wykorzystanie interfejsów API. API to część, która pozwala na połączenie dwóch mikroserwisów. Interfejsy API są bardzo podobne do stron internetowych. Podobnie jak strona internetowa, serwer wysyła do Ciebie kod reprezentujący stronę internetową. Twoja przeglądarka internetowa interpretuje ten kod i wyświetla stronę internetową.\nWeźmy przypadek biznesowy z modelem ML jako usługą. Załóżmy, że pracujesz dla firmy sprzedającej mieszkania w Bostonie. Chcesz zwiększać sprzedaż i oferować naszym klientom lepszą jakość usług dzięki nowej aplikacji mobilnej, z której może korzystać nawet 1 000 000 osób jednocześnie. Możemy to osiągnąć, udostępniając prognozę wartości domu, gdy użytkownik prosi o wycenę przez Internet.\n\nCzym jest serwowanie modelu ML\n\nSzkolenie dobrego modelu ML to TYLKO pierwsza część całego procesu: Musisz udostępnić swój model użytkownikom końcowym. Robisz to, zapewniając dostęp do modelu na swoim serwerze.\nAby udostępnić model potrzebujesz: modelu, interpretera, danych wsadowych.\nWażne metryki\n\n\nczas oczekiwania,\nkoszty,\nliczba zapytać w jednostce czasu\n\n\nUdostępnianie danych między dwoma lub więcej systemami zawsze było podstawowym wymogiem tworzenia oprogramowania – DevOps vs. MLOps.\n\nGdy wywołasz interfejs API, otrzyma on Twoje żądanie. Żądanie wyzwala kod do uruchomienia na serwerze i generuje odpowiedź odesłaną do Ciebie. Jeśli coś pójdzie nie tak, możesz nie otrzymać żadnej odpowiedzi lub otrzymać kod błędu jako kod stanu HTTP.\n\nKlient-Serwer: Klient (system A) przesyła żądanie przez HTTP do adresu URL hostowanego przez system B, który zwraca odpowiedź. Identycznie działa np przeglądarka internetowa. Żądanie jest kierowane do serwera WWW, który zwraca tekstową stronę HTML.\n\n\nBezstanowe: Żądanie klienta powinno zawierać wszystkie informacje niezbędne do udzielenia pełnej odpowiedzi.\n\nInterfejsy API można wywoływać za pomocą wielu różnych narzędzi. Czasami możesz nawet użyć przeglądarki internetowej. Narzędzia takie jak CURL wykonują zadanie w wierszu poleceń. Możesz używać narzędzi, takich jak Postman, do wywoływania interfejsów API za pomocą interfejsu użytkownika.\n\nCała komunikacja jest objęta ustalonymi zasadami i praktykami, które są nazywane protokołem HTTP.\n\n\n\nZapytanie - Request\n\nAdres URL (np. http://mydomain:8000/getapi?&val1=43&val2=3) zawiera:\n- domenę, \n- port, \n- dodatkowe ścieżki, \n- zapytanie\nMetody HTTP:\n- GET, \n- POST\nNagłówki HTTP zawierają:\n- informacje o autoryzacji, \n- cookies metadata\n\nCała informacja zawarta jest w Content-Type: application/json, text … Accept: application/json, Authorization: Basic abase64string, Tokens 4. Ciało zapytania\nNajczęściej wybieranym formatem dla wymiany informacji między serwisami jest format JavaScript Object Notation (JSON). Przypomina on pythonowy obiekt słownika - “klucz”: “wartość”.\n{\n\"RAD\": 1,\n\"PTRATIO\": 15.3, \"INDUS\": 2.31, \"B\": 396.9,\n\"ZN\": 18,\n\"DIS\": 4.09, \"CRIM\": 0.00632, \"RM\": 6.575, \n\"AGE\": 65.2, \"CHAS\": 0, \"NOX\": 0.538, \n\"TAX\": 296, \"LSTAT\": 4.98\n}\n\n\nOdpowiedź - Response\n\nTreść odpowiedzi przekazywana jest razem z nagłówkiem oraz statusem:\n\n200 OK\nContent-Encoding: gzip\nContent-Type: text/html; charset=utf-8\nDate: Mon, 18 Jul 2016 16:06:00 GMT Server: Apache\nPath=/;\n\nnp.: “Content-Type” =&gt; ”application/json; charset=utf-8”, ”Server” =&gt; ”Genie/Julia/1.8.5”\nTreść (ciało) odpowiedzi:\n\n{\":input\":{\"RAD\":1,\"PTRATIO\":15.3,\"INDUS\":2.31,.....}}, {\":prediction\":[29.919737211857683]}\n\nHTTP status code:\n\n\n200 OK - prawidłowe wykonanie zapytania,\n40X Access Denied\n50X Internal server error\n\n\nWyszukaj informacje czym jest REST API.\n\n\nWiedza:\n\nZna możliwości i obszary zastosowania procesowania danych w czasie rzeczywistym\nUmie wybrać strukturę IT dla danego problemu biznesowego\nRozumie potrzeby biznesowe podejmowania decyzji w bardzo krótkim czasie\n\n\n\nUmiejętności:\n\nRozróżnia typy danych strukturyzowanych jak i niestrukturyzowanych\n\n\n\nKompetencje:\n\nUtrwala umiejętność samodzielnego uzupełniania wiedzy teoretycznej jak i praktycznej w zakresie programowania, modelowania, nowych technologii informatycznych z wykorzystaniem analizy w czasie rzeczywistym.",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Mikroserwisy i komuniakcja przez REST API."
    ]
  },
  {
    "objectID": "lab/cw2.html",
    "href": "lab/cw2.html",
    "title": "Producent Apache Kafka",
    "section": "",
    "text": "Tą wersję można przejść również posiadając nowy obraz dockerowy i uruchomiony docker desktop na własnym komputerze.\n\nPrzejdź do przeglądarki i uruchom stronę ze środowiskiem (w przypadku Docker uruchom localhost:8888).\nUruchom (w jupyter lab za pomocą ikony terminala) nowy terminal\nPrzejdź do katalogu głównego i wypisz listę wszystkich elementów. Sprawdź czy na liście znajduje się katalog kafka.\ncd ~\nls -all\nUruchom polecenie sprawdzające listę topiców serwera Kafki bash     kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\nDodaj topic o nazwie streaming\nkafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streaming\nSprawdź listę tematów ponownie upewniając się, że posiadasz temat streaming\nUruchom nowy terminal na notatniku i utwórz producenta w konsoli generującego dane do nowego topicu\n\nkafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic streaming\nAby sprawdzić czy wysyłanie wiadomości działa uruchom kolejne okno terminala i wpisz następującą komendę realizującą konsumenta w konsoli:\nkafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic streaming --from-beginning\n\nPamiętaj aby uruchamiać komendy z odpowiedniego katalogu.\n\n\n%%file stream.py\n\nimport json\nimport random\nimport sys\nfrom datetime import datetime, timedelta\nfrom time import sleep\n\nfrom kafka import KafkaProducer\n\nif __name__ == \"__main__\":\n    SERVER = \"broker:9092\"\n\n    producer = KafkaProducer(\n        bootstrap_servers=[SERVER],\n        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n        api_version=(3, 7, 0),\n    )\n    \n    try:\n        while True:\n            \n            t = datetime.now() + timedelta(seconds=random.randint(-15, 0))\n            \n            message = {\n                \"time\" : str(t),\n                \"id\" : random.choice([\"a\", \"b\", \"c\", \"d\", \"e\"]),\n                \"values\" : random.randint(0,100)\n            }\n            \n            \n            producer.send(\"streaming\", value=message)\n            sleep(1)\n    except KeyboardInterrupt:\n        producer.close()",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Producent Apache Kafka"
    ]
  },
  {
    "objectID": "lab/cw2.html#wersja-z-dostępem-do-środowiska",
    "href": "lab/cw2.html#wersja-z-dostępem-do-środowiska",
    "title": "Producent Apache Kafka",
    "section": "",
    "text": "Tą wersję można przejść również posiadając nowy obraz dockerowy i uruchomiony docker desktop na własnym komputerze.\n\nPrzejdź do przeglądarki i uruchom stronę ze środowiskiem (w przypadku Docker uruchom localhost:8888).\nUruchom (w jupyter lab za pomocą ikony terminala) nowy terminal\nPrzejdź do katalogu głównego i wypisz listę wszystkich elementów. Sprawdź czy na liście znajduje się katalog kafka.\ncd ~\nls -all\nUruchom polecenie sprawdzające listę topiców serwera Kafki bash     kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\nDodaj topic o nazwie streaming\nkafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streaming\nSprawdź listę tematów ponownie upewniając się, że posiadasz temat streaming\nUruchom nowy terminal na notatniku i utwórz producenta w konsoli generującego dane do nowego topicu\n\nkafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic streaming\nAby sprawdzić czy wysyłanie wiadomości działa uruchom kolejne okno terminala i wpisz następującą komendę realizującą konsumenta w konsoli:\nkafka/bin/kafka-console-consumer.sh --bootstrap-server broker:9092 --topic streaming --from-beginning\n\nPamiętaj aby uruchamiać komendy z odpowiedniego katalogu.\n\n\n%%file stream.py\n\nimport json\nimport random\nimport sys\nfrom datetime import datetime, timedelta\nfrom time import sleep\n\nfrom kafka import KafkaProducer\n\nif __name__ == \"__main__\":\n    SERVER = \"broker:9092\"\n\n    producer = KafkaProducer(\n        bootstrap_servers=[SERVER],\n        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n        api_version=(3, 7, 0),\n    )\n    \n    try:\n        while True:\n            \n            t = datetime.now() + timedelta(seconds=random.randint(-15, 0))\n            \n            message = {\n                \"time\" : str(t),\n                \"id\" : random.choice([\"a\", \"b\", \"c\", \"d\", \"e\"]),\n                \"values\" : random.randint(0,100)\n            }\n            \n            \n            producer.send(\"streaming\", value=message)\n            sleep(1)\n    except KeyboardInterrupt:\n        producer.close()",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Producent Apache Kafka"
    ]
  },
  {
    "objectID": "lab/cw2.html#wersja-obraz-docker",
    "href": "lab/cw2.html#wersja-obraz-docker",
    "title": "Producent Apache Kafka",
    "section": "Wersja obraz Docker",
    "text": "Wersja obraz Docker\n\nPrzejdź do katalogu jupyterlab i uruchom obraz poleceniem\ndocker compose up\nOtwórz nowy terminal i sprawdź listę topiców\ndocker exec broker kafka-topics --list --bootstrap-server broker:9092\nLista ta zazwyczaj jest pusta.\ndodaj topic o nazwie streaming\n\ndocker exec broker kafka-topics --bootstrap-server broker:9092 --create --topic streaming\n\nSprawdź listę tematów ponownie upewniając się, że posiadasz temat streaming\nUruchom nowy terminal na swoim komputerze i utwórz producenta generującego dane do nowego topicu\n\ndocker exec --interactive --tty broker kafka-console-producer --bootstrap-server broker:9092 --topic streaming\nAby sprawdzić czy wysyłanie wiadomości działa uruchom kolejne okno terminala i wpisz następującą komendę realizującą consumenta:\ndocker exec --interactive --tty broker kafka-console-consumer --bootstrap-server broker:9092 --topic streaming --from-beginning",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Producent Apache Kafka"
    ]
  },
  {
    "objectID": "lab/cw1.html",
    "href": "lab/cw1.html",
    "title": "Środowisko produkcyjne z modelem ML",
    "section": "",
    "text": "Naszym zadaniem jest wystawić aplikację w pythonie realizującą zadania REST API. Na ządanie klienta serwer udzieli odpowiedzi na podstawie predykcji wygenerowanej z jakiegoś modelu.\nObraz ten zostanie skonterenryzowany z wykorzystaniem pliku Dockerfile dzięki czemu uruchomienie serwera stanie się mozliwe nie zaleznie od platformy.\nNasze zadanie zrealizujemy z wykorzystaniem biblioteki Flask w wersji 3.0.3.\nIstnieją inne biblioteki realizujące to zadanie. &gt; Sprawdź samodzielnie w domu czy potrafisz je uzyć, albo przynajmniej przeczytać kod.",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Środowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "lab/cw1.html#kod-minimalnej-aplikacji-flask",
    "href": "lab/cw1.html#kod-minimalnej-aplikacji-flask",
    "title": "Środowisko produkcyjne z modelem ML",
    "section": "Kod minimalnej aplikacji flask",
    "text": "Kod minimalnej aplikacji flask\nNaszą aplikację chcemy uruchomić lokalnie a potem w łatwy sposób przenieść i wykonać na dowolnym komputerze. Z tej przyczyny naturalnym rozwiązaniem jest zapis kodu do pliku z rozszerzeniem .py.\nW celu autozapisu kodu aplikacji do pliku app.py wykorzystamy magiczną komendę %%file plik.py.\n\n%%file app.py\nfrom flask import Flask\n\n# Create a flask\napp = Flask(__name__)\n\n# Create an API end point\n@app.route('/')\ndef say_hello():\n    return \"Hello World\"\n\nif __name__ == '__main__':\n    app.run() # domyślnie ustawia localhost i port 5000\n    # app.run(host='0.0.0.0', port=8000)\n\n\nUwaga! w dokumentacji Flask w przykładowym kodzie nie występują ostatnie dwie liniki uruchamiające serwer. Ponadto poleceniem uruchamiającym serwer jest flask run a nie python app.py.\n\nWyjaśnijmy co zawiera przykładowy kod.\n\nfrom flask import Flask Załadowanie biblioteki\napp = Flask(__name__) utworzenie interfejsu serwera API\nkod podstrony z wykorzystaniem dekoratora\n\n@app.route('/')\ndef say_hello():\n    return \"Hello World\"\nW celu pokazania jak działa dekorator zdefiniujmy następującą funkcję:\n\ndef make_pretty(func):\n    def inner():\n        print(\"decorator działa\")\n        func()\n    return inner()\n\nNastępnie funkcja testowa\n\ndef test():\n    print(\"abc\")\n\nmake_pretty(test)\n\ndecorator działa\nabc\n\n\nAle mozna rowniez inaczej\n\n@make_pretty\ndef test2():\n    print(\"test2\")\n\ndecorator działa\ntest2\n\n\n\n@make_pretty\ndef test3():\n    print(\"jeszcze cos\")\n\ndecorator działa\njeszcze cos",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Środowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "lab/cw1.html#środowisko-python",
    "href": "lab/cw1.html#środowisko-python",
    "title": "Środowisko produkcyjne z modelem ML",
    "section": "Środowisko Python",
    "text": "Środowisko Python\nAby kod aplikacji app.py mógł zostać uruchomiony potrzebujemy aby na naszym komputerze istniał jakiś interpreter języka Python. Samo posiadanie interpretatora nie jest jeszcze wystarczające dla naszej aplikacji. Do pełnego uruchomienia potrzebujemy wygenerować środowisko (najlepiej wirtualne) w który dostępne będą wszystkie potrzebne biblioteki (np. flask).\n\nuwaga: wszystkie polecenia terminala dotyczyć będą wersji linux/mac os.\n\nW pierwszej kolejności sprawdź czy dostępne są polecenia pozwalające realizować kod pythonowy.\nwhich python\nwhich python3\nwhich pip \nwhich pip3\nWszystkie te polecenia powinny wskazyać na folder z domyślnym środowiskiem Pythona.\nWygeneruj i uruchom środowisko wirtualne lokalnie wpisując w terminalu:\npython3 -m venv .venv\nsource .venv/bin/activate\n\nDobra praktyka: środowisko python to nic innego jak katalog. W naszej wersji to katalog ukryty o nazwie .venv. Jeśli skopiujesz ten katalog gdzie indziej przestanie pełnić on swoją funkcję środowiska python. Dlatego jego odtworzenie nie polega na jego kopiowaniu. Jeśli Twój projekt jest powiązany ze środowiskiem kontroli wersji GIT zadbaj aby katalog środowiska nie był dodawany do repozytorium. Mozesz wykonać to działanie dodając odpowiedni wpis do pliki .gitignore\n\nPosiadając utworzone nowe środowisko sprawdź jakie biblioteki się w nim znajdują.\npip list \n\nPackage    Version\n---------- -------\npip        23.2.1\npyspark    3.4.1\nsetuptools 65.5.0\nMozemy ponownie sprawdzić polecenia python i pip\nwhich python\nwhich pip \nDomyślnie powinny pojawić się biblioteki pip oraz setuptools (pyspark pochodzi z naszego wewnętrzengo obrazu).\nDoinstaluj bibliotekę flask\npip install flask\npip list \nPackage      Version\n------------ -------\nblinker      1.7.0\nclick        8.1.7\nFlask        3.0.3\nitsdangerous 2.1.2\nJinja2       3.1.3\nMarkupSafe   2.1.5\npip          23.2.1\npyspark      3.4.1\nsetuptools   65.5.0\nWerkzeug     3.0.2\nJak widać instalacja biblioteki flask wymusiła doinstalowanie równiez innych pakietów.\nJedyną mozliwością przeniesienia środowiska python jest jego instalacja na nowej maszynie i instalacja wszystkich pakietów ręcznie. Aby jednak nie instalować kazdego pakietu osobno mozemy wykorzystać plik requirements.txt konfiguracyjny z listą pakietów.\n\nPamiętaj - kazdy pakiet powinien zawierać nr wersji pakietu. W innym przypadku moze okazać się, ze nowe werjse pakietów powodują brak obsługi twojego kodu.\n\nAby utworzyć plik konfiguracyjny uzyj polecenia w terminalu:\npip freeze &gt;&gt; requirements.txt\nTak wygenerowany plik mozesz uzywać na dowolnej maszynie do instalacji i odtworzenia potrzebnego środowiska wykonawczego python.\n\nDygresja. W momencie przygotowywania materiałów Flask był w wersji 3.0.1 - dziś juz realizowany jest w wersji 3.0.3.\n\nInstalacja pakietów z pliku odbywa się z wykorzystaniem polecenia:\npip install -r requierements.txt\nMamy teraz dwa pliki: app.py, i requirements.txt. Przenosząc je do dowolnego projektu na serwerach github jesteśmy w stanie uruchomić naszą aplikację wszędzie tam gdzie dostępny będzie interpreter python na którym mozemy utworzyć nowe wirtualne środowisko i zainstalować biblioteki z pliku requirements.txt.\nDo pełnej automatyzacji przydałaby się jeszcze mozliwość uruchomienia środowiska python na dowolnej maszynie.\nW tym celu utwórz plik Dockerfile:\n\n%%file Dockerfile\nFROM python:3.11-slim-buster\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\n\nRUN pip install -r requirements.txt\n\nCOPY app.py .\n\nENV FLASK_APP=app\n\nEXPOSE 8000\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\nPowyzszy plik pozwala w docker desktop uruchomić obraz wykorzystujący podstawowy system operacyjny (tutaj linux) wraz z podstawowym środowiskiem python3.11.\nPonadto plik ten kopiuje potrzebne pliki (app.py, requirements.txt) na obraz dockera.\nPolecenie RUN pozwala uruchomić dowolne polecenie bash wewnątrz obrazu dockera.\nPolecenie CMD pozwala uruchomić polecenie uruchamiające serwer w trybie tak by nie zamknąć tego polecenia.\nOstatnią informacją jest ustalenie portu na 8000.\n# utworzenie kontenera na podstawie pliku Dockerfile\ndocker build -t modelML .\n# uruchomienie kontenera\ndocker run -p 8000:8000 modelML",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Środowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "lab/cw1.html#uruchomienie-serwera-lokalnie",
    "href": "lab/cw1.html#uruchomienie-serwera-lokalnie",
    "title": "Środowisko produkcyjne z modelem ML",
    "section": "Uruchomienie serwera lokalnie",
    "text": "Uruchomienie serwera lokalnie\nUruchomienie serwera moze odbyć się na przynajmniej na dwa sposoby.\n\nUruchomienie serwera przez terminal\npython app.py\nlub (jeśli nie ma kodu app.run() uruchamiającego serwer.)\nflask run \nPowinna pojawić się informacja podobna do ponizszej:\n * Serving Flask app 'app'\n * Debug mode: off\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:5000\nPress CTRL+C to quit\n\n\nUruchomienie serwera przez notatnik\nBezpośrenie uruchomienia kodu w notatniku spowoduje uruchomienie serwera i zatrzymanie jakiejkolwiek mozliwości realizacji kodu. Aby tego uniknąć mozesz wykorzystać bibliotekę subprocess.\n\nimport subporcess\n\np = subprocess.Popen([\"python\", \"app.py\"])\n\nJeśli potrzebujemy zamknąć subprocess wykonaj:\n\np.kill()\n\nPosiadając uruchomiony serwer mozesz odpytac serwer wykorzystując:\ncurl localhost:5000\nalbo kod w notatniku:\n\nimport requests\n\nresponse = requests.get(\"http://127.0.0.1:5000/\")\n\nprint(response.content) # Hello World\nprint(response.status_code) # 200",
    "crumbs": [
      "222890-D",
      "Ćwiczenia",
      "Środowisko produkcyjne z modelem ML"
    ]
  },
  {
    "objectID": "wyklad4.html",
    "href": "wyklad4.html",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Architektura przesyłania strumieniowego to określony zestaw technologii, które współpracują ze sobą w celu obsługi przetwarzania strumieniowego, co jest praktyką podejmowania działań na serii danych w momencie ich tworzenia. W wielu nowoczesnych wdrożeniach Apache Kafka działa jako magazyn danych przesyłanych strumieniowo, a następnie wiele procesorów strumieniowych może działać na danych przechowywanych w Kafce w celu wygenerowania wielu danych wyjściowych. Niektóre architektury przesyłania strumieniowego obejmują przepływy pracy zarówno do przetwarzania strumieniowego, jak i przetwarzania wsadowego, które obejmują inne technologie do obsługi przetwarzania wsadowego na dużą skalę lub wykorzystują Kafkę jako magazyn centralny, jak określono w architekturze Kappa.\nDoskonała architektura przetwarzania danych w czasie rzeczywistym musi być odporna na błędy i skalowalna; musi obsługiwać aktualizacje wsadowe i przyrostowe oraz być rozszerzalna.\nNa początku badamy dwie podstawowe architektury przetwarzania danych, Lambda i Kappa, które stanowią podstawę różnych aplikacji korporacyjnych.\n\n\nArchitektura Lambda obejmuje warstwę wsadową (batch layer), warstwę strumieniowa (stream layer) i warstwę serwowania.\nWarstwa wsadowa działa na pełnych danych, dzięki czemu system może generować najdokładniejsze wyniki. Jednak wyniki są okupione dużymi opóźnieniami wynikającymi z długiego czasu obliczeń. Warstwa wsadowa przechowuje surowe dane w miarę ich nadejścia i oblicza widoki wsadowe do wykorzystania. Naturalnie procesy wsadowe będą występować w pewnych odstępach czasu i będą długotrwałe. Zakres danych wynosi od godzin do kilku lat.\nWarstwa strumieniowa:\n\ngeneruje wyniki z małymi opóźnieniami i w czasie zbliżonym do rzeczywistego.\noblicza widoki w czasie rzeczywistym w celu uzupełnienia widoków wsadowych.\nodbiera napływające dane i aktualizuje wyniki warstwy wsadowej. Koszt obliczeń jest znacznie obniżony dzięki algorytmom przyrostowym zaimplementowanym w warstwie szybkości.\n\nWidoki wsadowe mogą być przetwarzane przy użyciu bardziej złożonych lub kosztownych reguł i mogą mieć lepszą jakość danych i mniej przekrzywień, podczas gdy widoki w czasie rzeczywistym zapewniają bieżący dostęp do najnowszych możliwych danych.\nWreszcie warstwa serwująca umożliwia różne zapytania o wyniki przesłane z warstw wsadowych i szybkich. Dane wyjściowe z warstwy wsadowej w postaci widoków wsadowych i warstwy szybkości w postaci opinii w czasie zbliżonym do rzeczywistego są przekazywane do warstwy obsługującej, która wykorzystuje te dane do obsługi oczekujących zapytań na zasadzie ad-hoc.\n  Implementacja:  \nDobre bo:\n\nDobra równowaga między szybkością, niezawodnością i skalowalnością.\nDostęp do wyników zarówno w czasie rzeczywistym, jak i offline, bardzo dobrze pokrywa wiele scenariuszy analizy danych.\nDostęp do pełnego zestawu danych w oknie wsadowym może przynieść określone optymalizacje, które sprawią, że Lambda będzie wydajniejsza i jeszcze prostsza do wdrożenia.\n\nKiepskie gdy:\n\nWewnętrzna logika przetwarzania jest taka sama (warstwy wsadowe i warstwy czasu rzeczywistego) - wiele zduplikowanych modułów i kodowania.\nZbiór danych modelowany za pomocą architektury Lambda jest trudny do migracji i reorganizacji.\n\n\n\n\nArchitektura Kappa to architektura oprogramowania używana do przetwarzania danych przesyłanych strumieniowo. Głównym założeniem Architektury Kappa jest możliwość wykonywania przetwarzania w czasie rzeczywistym i przetwarzania wsadowego, zwłaszcza w celach analitycznych, za pomocą jednego stosu technologicznego. Opiera się na architekturze przesyłania strumieniowego, w której przychodzące serie danych są najpierw przechowywane w silniku przesyłania wiadomości, takim jak Apache Kafka. Stamtąd silnik przetwarzania strumienia odczyta dane, przekształci je w format nadający się do analizy, a następnie zapisze je w analitycznej bazie danych, aby użytkownicy końcowi mogli wyszukiwać.\nArchitektura Kappa obsługuje analizy (prawie) w czasie rzeczywistym, gdy dane są odczytywane i przekształcane natychmiast po umieszczeniu ich w silniku przesyłania komunikatów. Dzięki temu najnowsze dane są szybko dostępne dla zapytań użytkowników końcowych. Obsługuje również analizę historyczną, odczytując zapisane dane przesyłane strumieniowo z mechanizmu przesyłania wiadomości później w sposób wsadowy, aby utworzyć dodatkowe możliwe do analizy dane wyjściowe dla większej liczby typów analiz.\nArchitektura Kappa jest prostszą alternatywą dla architektury Lambda, ponieważ wykorzystuje ten sam stos technologii do obsługi strumienia w czasie rzeczywistym i historycznego przetwarzania wsadowego. Obie architektury obejmują przechowywanie danych historycznych w celu umożliwienia analiz na dużą skalę. Obie architektury są również pomocne w rozwiązywaniu problemów związanych z „tolerancją błędów ludzkich”, w których problemy z kodem przetwarzania (błędy lub znane ograniczenia) można przezwyciężyć, aktualizując kod i ponownie uruchamiając go na danych historycznych. Główna różnica w stosunku do architektury Kappa polega na tym, że wszystkie dane są traktowane jako strumień, więc silnik przetwarzania strumienia działa jako jedyny silnik transformacji danych.\n \nImplementation Example:  \n\n\n\n\nAplikacje mogą odczytywać i zapisywać bezpośrednio do Kafki zgodnie z rozwojem. W przypadku istniejących źródeł zdarzeń detektory są teraz przyzwyczajone do przesyłania strumieniowego raportów z dzienników bazy danych, co eliminuje konieczność przetwarzania wsadowego podczas ruchu przychodzącego, co skutkuje mniejszą liczbą zasobów.\nZapytania muszą uwzględniać tylko jedną lokalizację serwowania, zamiast sprawdzać widoki partii i szybkości.\n\n\n\n\n\nniełatwe do wdrożenia, zwłaszcza w przypadku odtwarzania danych.\n\n\n\n\nObie architektury obsługują analizy w czasie rzeczywistym i historyczne w jednym środowisku. Jednak istotną zaletą architektury Kappa w porównaniu z architekturą Lambda jest to, że umożliwia ona zbudowanie systemu przesyłania strumieniowego i przetwarzania wsadowego na jednej technologii. Oznacza to, że możesz zbudować aplikację przetwarzającą strumienie do obsługi danych w czasie rzeczywistym, a jeśli musisz zmodyfikować dane wyjściowe, zaktualizuj swój kod, a następnie ponownie uruchom go na danych w mechanizmie przesyłania komunikatów w sposób wsadowy. Jak sugeruje architektura Lambda, nie ma osobnej technologii do obsługi przetwarzania wsadowego.\nPrzy wystarczająco szybkim silniku przetwarzania strumieniowego możesz nie potrzebować innej technologii zoptymalizowanej pod kątem przetwarzania wsadowego. Odczytujesz równolegle przechowywane dane przesyłane strumieniowo (zakładając, że dane w Kafce są odpowiednio podzielone na osobne kanały lub „partycje”) i przekształcasz dane tak, jakby pochodziły ze źródła strumieniowego. W przypadku niektórych środowisk możliwe do przeanalizowania dane wyjściowe można utworzyć na żądanie. Gdy nowe zapytanie zostanie przesłane przez użytkownika końcowego, dane mogą zostać przekształcone ad hoc, aby uzyskać optymalną odpowiedź na to zapytanie. Ponownie wymaga to szybkiego silnika przetwarzania strumieniowego, aby zapewnić małe opóźnienia.\nChociaż architektura Lambda nie określa technologii, których należy użyć, komponent przetwarzania wsadowego jest często wykonywany na platformie big data z wykorzystaniem Apache Hadoop. Rozproszony system plików Hadoop (HDFS) umożliwia ekonomiczne przechowywanie „surowych danych”, które można przekształcić za pomocą narzędzi Hadoop w format umożliwiający analizę. Podczas gdy Hadoop jest używany w komponencie systemu do przetwarzania wsadowego, oddzielny silnik przeznaczony do przetwarzania strumieniowego jest używany w komponencie analitycznym w czasie rzeczywistym. Jednak jedną z zalet architektury Lambda jest to, że znacznie większe zestawy danych (w zakresie petabajtów) można przechowywać i przetwarzać wydajniej w Hadoop w celu analizy historycznej na dużą skalę.",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Architektury strumieniowania danych"
    ]
  },
  {
    "objectID": "wyklad4.html#architektury-strumieniowania-danych",
    "href": "wyklad4.html#architektury-strumieniowania-danych",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "",
    "text": "Architektura przesyłania strumieniowego to określony zestaw technologii, które współpracują ze sobą w celu obsługi przetwarzania strumieniowego, co jest praktyką podejmowania działań na serii danych w momencie ich tworzenia. W wielu nowoczesnych wdrożeniach Apache Kafka działa jako magazyn danych przesyłanych strumieniowo, a następnie wiele procesorów strumieniowych może działać na danych przechowywanych w Kafce w celu wygenerowania wielu danych wyjściowych. Niektóre architektury przesyłania strumieniowego obejmują przepływy pracy zarówno do przetwarzania strumieniowego, jak i przetwarzania wsadowego, które obejmują inne technologie do obsługi przetwarzania wsadowego na dużą skalę lub wykorzystują Kafkę jako magazyn centralny, jak określono w architekturze Kappa.\nDoskonała architektura przetwarzania danych w czasie rzeczywistym musi być odporna na błędy i skalowalna; musi obsługiwać aktualizacje wsadowe i przyrostowe oraz być rozszerzalna.\nNa początku badamy dwie podstawowe architektury przetwarzania danych, Lambda i Kappa, które stanowią podstawę różnych aplikacji korporacyjnych.\n\n\nArchitektura Lambda obejmuje warstwę wsadową (batch layer), warstwę strumieniowa (stream layer) i warstwę serwowania.\nWarstwa wsadowa działa na pełnych danych, dzięki czemu system może generować najdokładniejsze wyniki. Jednak wyniki są okupione dużymi opóźnieniami wynikającymi z długiego czasu obliczeń. Warstwa wsadowa przechowuje surowe dane w miarę ich nadejścia i oblicza widoki wsadowe do wykorzystania. Naturalnie procesy wsadowe będą występować w pewnych odstępach czasu i będą długotrwałe. Zakres danych wynosi od godzin do kilku lat.\nWarstwa strumieniowa:\n\ngeneruje wyniki z małymi opóźnieniami i w czasie zbliżonym do rzeczywistego.\noblicza widoki w czasie rzeczywistym w celu uzupełnienia widoków wsadowych.\nodbiera napływające dane i aktualizuje wyniki warstwy wsadowej. Koszt obliczeń jest znacznie obniżony dzięki algorytmom przyrostowym zaimplementowanym w warstwie szybkości.\n\nWidoki wsadowe mogą być przetwarzane przy użyciu bardziej złożonych lub kosztownych reguł i mogą mieć lepszą jakość danych i mniej przekrzywień, podczas gdy widoki w czasie rzeczywistym zapewniają bieżący dostęp do najnowszych możliwych danych.\nWreszcie warstwa serwująca umożliwia różne zapytania o wyniki przesłane z warstw wsadowych i szybkich. Dane wyjściowe z warstwy wsadowej w postaci widoków wsadowych i warstwy szybkości w postaci opinii w czasie zbliżonym do rzeczywistego są przekazywane do warstwy obsługującej, która wykorzystuje te dane do obsługi oczekujących zapytań na zasadzie ad-hoc.\n  Implementacja:  \nDobre bo:\n\nDobra równowaga między szybkością, niezawodnością i skalowalnością.\nDostęp do wyników zarówno w czasie rzeczywistym, jak i offline, bardzo dobrze pokrywa wiele scenariuszy analizy danych.\nDostęp do pełnego zestawu danych w oknie wsadowym może przynieść określone optymalizacje, które sprawią, że Lambda będzie wydajniejsza i jeszcze prostsza do wdrożenia.\n\nKiepskie gdy:\n\nWewnętrzna logika przetwarzania jest taka sama (warstwy wsadowe i warstwy czasu rzeczywistego) - wiele zduplikowanych modułów i kodowania.\nZbiór danych modelowany za pomocą architektury Lambda jest trudny do migracji i reorganizacji.\n\n\n\n\nArchitektura Kappa to architektura oprogramowania używana do przetwarzania danych przesyłanych strumieniowo. Głównym założeniem Architektury Kappa jest możliwość wykonywania przetwarzania w czasie rzeczywistym i przetwarzania wsadowego, zwłaszcza w celach analitycznych, za pomocą jednego stosu technologicznego. Opiera się na architekturze przesyłania strumieniowego, w której przychodzące serie danych są najpierw przechowywane w silniku przesyłania wiadomości, takim jak Apache Kafka. Stamtąd silnik przetwarzania strumienia odczyta dane, przekształci je w format nadający się do analizy, a następnie zapisze je w analitycznej bazie danych, aby użytkownicy końcowi mogli wyszukiwać.\nArchitektura Kappa obsługuje analizy (prawie) w czasie rzeczywistym, gdy dane są odczytywane i przekształcane natychmiast po umieszczeniu ich w silniku przesyłania komunikatów. Dzięki temu najnowsze dane są szybko dostępne dla zapytań użytkowników końcowych. Obsługuje również analizę historyczną, odczytując zapisane dane przesyłane strumieniowo z mechanizmu przesyłania wiadomości później w sposób wsadowy, aby utworzyć dodatkowe możliwe do analizy dane wyjściowe dla większej liczby typów analiz.\nArchitektura Kappa jest prostszą alternatywą dla architektury Lambda, ponieważ wykorzystuje ten sam stos technologii do obsługi strumienia w czasie rzeczywistym i historycznego przetwarzania wsadowego. Obie architektury obejmują przechowywanie danych historycznych w celu umożliwienia analiz na dużą skalę. Obie architektury są również pomocne w rozwiązywaniu problemów związanych z „tolerancją błędów ludzkich”, w których problemy z kodem przetwarzania (błędy lub znane ograniczenia) można przezwyciężyć, aktualizując kod i ponownie uruchamiając go na danych historycznych. Główna różnica w stosunku do architektury Kappa polega na tym, że wszystkie dane są traktowane jako strumień, więc silnik przetwarzania strumienia działa jako jedyny silnik transformacji danych.\n \nImplementation Example:  \n\n\n\n\nAplikacje mogą odczytywać i zapisywać bezpośrednio do Kafki zgodnie z rozwojem. W przypadku istniejących źródeł zdarzeń detektory są teraz przyzwyczajone do przesyłania strumieniowego raportów z dzienników bazy danych, co eliminuje konieczność przetwarzania wsadowego podczas ruchu przychodzącego, co skutkuje mniejszą liczbą zasobów.\nZapytania muszą uwzględniać tylko jedną lokalizację serwowania, zamiast sprawdzać widoki partii i szybkości.\n\n\n\n\n\nniełatwe do wdrożenia, zwłaszcza w przypadku odtwarzania danych.\n\n\n\n\nObie architektury obsługują analizy w czasie rzeczywistym i historyczne w jednym środowisku. Jednak istotną zaletą architektury Kappa w porównaniu z architekturą Lambda jest to, że umożliwia ona zbudowanie systemu przesyłania strumieniowego i przetwarzania wsadowego na jednej technologii. Oznacza to, że możesz zbudować aplikację przetwarzającą strumienie do obsługi danych w czasie rzeczywistym, a jeśli musisz zmodyfikować dane wyjściowe, zaktualizuj swój kod, a następnie ponownie uruchom go na danych w mechanizmie przesyłania komunikatów w sposób wsadowy. Jak sugeruje architektura Lambda, nie ma osobnej technologii do obsługi przetwarzania wsadowego.\nPrzy wystarczająco szybkim silniku przetwarzania strumieniowego możesz nie potrzebować innej technologii zoptymalizowanej pod kątem przetwarzania wsadowego. Odczytujesz równolegle przechowywane dane przesyłane strumieniowo (zakładając, że dane w Kafce są odpowiednio podzielone na osobne kanały lub „partycje”) i przekształcasz dane tak, jakby pochodziły ze źródła strumieniowego. W przypadku niektórych środowisk możliwe do przeanalizowania dane wyjściowe można utworzyć na żądanie. Gdy nowe zapytanie zostanie przesłane przez użytkownika końcowego, dane mogą zostać przekształcone ad hoc, aby uzyskać optymalną odpowiedź na to zapytanie. Ponownie wymaga to szybkiego silnika przetwarzania strumieniowego, aby zapewnić małe opóźnienia.\nChociaż architektura Lambda nie określa technologii, których należy użyć, komponent przetwarzania wsadowego jest często wykonywany na platformie big data z wykorzystaniem Apache Hadoop. Rozproszony system plików Hadoop (HDFS) umożliwia ekonomiczne przechowywanie „surowych danych”, które można przekształcić za pomocą narzędzi Hadoop w format umożliwiający analizę. Podczas gdy Hadoop jest używany w komponencie systemu do przetwarzania wsadowego, oddzielny silnik przeznaczony do przetwarzania strumieniowego jest używany w komponencie analitycznym w czasie rzeczywistym. Jednak jedną z zalet architektury Lambda jest to, że znacznie większe zestawy danych (w zakresie petabajtów) można przechowywać i przetwarzać wydajniej w Hadoop w celu analizy historycznej na dużą skalę.",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Architektury strumieniowania danych"
    ]
  },
  {
    "objectID": "wyklad4.html#publikujsubskrybuj",
    "href": "wyklad4.html#publikujsubskrybuj",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Publikuj/Subskrybuj",
    "text": "Publikuj/Subskrybuj\nSystem przesyłania wiadomości „Publikuj/Subskrybuj” ma kluczowe znaczenie dla aplikacji opartych na danych. Komunikaty Pub/Sub to wzorzec charakteryzujący się tym, że nadawca (publikujący) fragmentu danych (wiadomości) nie kieruje go wprost do odbiorcy. pub/sub to systemy, które często posiadają brokera czyli centralny punkt, w którym znajdują się wiadomości.",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Architektury strumieniowania danych"
    ]
  },
  {
    "objectID": "wyklad4.html#apache-kafka",
    "href": "wyklad4.html#apache-kafka",
    "title": "Analiza danych w czasie rzeczywistym",
    "section": "Apache Kafka",
    "text": "Apache Kafka\nNa witrynie Kafki znajdziesz definicję:\n\nRozproszona platforma streamingowa\n\nCo to jest „platforma rozproszonego przesyłania strumieniowego”?\nNajpierw chcę przypomnieć, czym jest „strumień”. Strumienie to po prostu nieograniczone dane, dane, które nigdy się nie kończą. Ciągle ich przybywa i możesz przetwarzać je w czasie rzeczywistym.\nA „rozproszone”? Rozproszony oznacza, że ​​Kafka działa w klastrze, a każdy węzeł w grupie nazywa się Brokerem. Ci brokerzy to po prostu serwery wykonujące kopię Apache Kafka.\nTak więc Kafka to zestaw współpracujących ze sobą maszyn, aby móc obsługiwać i przetwarzać nieograniczone dane w czasie rzeczywistym.\nJego rozproszona architektura jest jednym z powodów, dla których Kafka stał się tak sławny. Brokerzy sprawiają, że jest odporny, niezawodny, skalowalny i odporny na błędy. Ale dlaczego panuje błędne przekonanie, że Kafka to kolejny „kolejkowy system przesyłania wiadomości”?\nAby odpowiedzieć na tę odpowiedź, musimy najpierw wyjaśnić, jak działa kolejkowe przesyłanie wiadomości.\n\nKolejkowy system przesyłania wiadomości\nPrzesyłanie wiadomości, to po prostu czynność wysyłania wiadomości z jednego miejsca do drugiego. Ma trzech głównych “aktorów”:\n\nProducent: Który tworzy i wysyła komunikaty do jednej lub więcej kolejek;\nKolejka: struktura danych bufora, która odbiera (od producentów) i dostarcza komunikaty (do konsumentów) w sposób FIFO (First-In-First-Out). Po otrzymaniu powiadomienia jest ono na zawsze usuwane z kolejki; nie ma szans na odzyskanie go;\nKonsument: subskrybuje jedną lub więcej kolejek i otrzymuje ich wiadomości po opublikowaniu.\n\nI to jest to; tak działa przesyłanie wiadomości. Jak widać, nie ma tu nic o strumieniach, czasie rzeczywistym czy klastrach.\n\n\nArchitektura Kafki\nDużo informacji znajdziesz pod tym linkiem.\nTeraz, gdy wiemy, jak działa przesyłanie wiadomości, zanurzmy się w świat Kafki. W Kafce mamy też „Producentów” i „Konsumentów”; działają w bardzo podobny sposób, jak w kolejkowych systemach, produkując i konsumując komunikaty.\n \nJak widać, jest to bardzo podobne do tego, o czym rozmawialiśmy o przesyłaniu wiadomości, ale tutaj nie mamy pojęcia „kolejki”. Zamiast tego mamy „Tematy” (Topic).\n„Temat” to szczególny typ strumienia danych; jest bardzo podobny do kolejki, odbiera i dostarcza wiadomości, ale jest kilka pojęć, które musimy zrozumieć w odniesieniu do tematów:\n\nTemat jest podzielony na partycje; każdy temat może mieć jedną lub więcej partycji i musimy określić tę liczbę podczas tworzenia topicu. Możesz sobie wyobrazić topic jako folder w systemie operacyjnym, a każdy folder wewnątrz niego jako partycję.\nKażda wiadomość zostanie zapisana na dysku brokera i otrzyma offset (unikalny identyfikator). Offset jest unikalny na poziomie partycji; każda partycja ma swój własny zbiór offsetów. To jeszcze jeden powód, który sprawia, że ​​Kafka jest tak wyjątkowa, przechowuje wiadomości na dysku (jak baza danych, a w rzeczywistości Kafka też jest bazą danych), aby w razie potrzeby odzyskać je później. W odróżnieniu od systemu przesyłania wiadomości, gdzie wiadomość jest usuwana po zużyciu;\nKosumenci używają offsetu do czytania wiadomości, od najstarszej do najnowszej. W przypadku awarii konsumenta zacznie odczytywać końcowe wartości realizowane po nawiązaniu połączenia.\n\n \n\n\nBrokerzy\nJak wspomniano wcześniej, Kafka działa w sposób rozproszony. W razie potrzeby klaster Kafka może zawierać wielu brokerów.\n \nKażdy broker w klastrze jest identyfikowany przez identyfikator i zawiera co najmniej jedną partycję tematyczną. Aby skonfigurować liczbę partycji w każdym brokerze, podczas tworzenia tematu musimy skonfigurować coś, co nazywa się współczynnikiem replikacji. Powiedzmy, że mamy trzech brokerów w naszym klastrze, temat z trzema partycjami i współczynnikiem replikacji równym trzy; w takim przypadku każdy broker będzie odpowiedzialny za jedną sekcję emisji.\nJak widać na powyższym obrazku, \\(Topic_1\\) ma trzy partycje; każdy broker jest odpowiedzialny za sekcję tematu, więc współczynnik replikacji \\(Topic_1\\) wynosi trzy. Liczba partycji musi być zgodna z liczbą brokerów; w ten sposób każdy broker będzie odpowiedzialny za jedną sekcję tematu.\n\n\nProducenci\nPodobnie jak w świecie kolejkowych systemów, „Producenci” w Kafce to ci, którzy tworzą i wysyłają wiadomości do tematów. Jak wspomniano wcześniej, wiadomości są wysyłane w sposób okrężny. Przykład: wiadomość 01 trafia do partycji 0 tematu 1, a wiadomość 02 do partycji 1 tego samego tematu. Oznacza to, że nie możemy zagwarantować, że wiadomości stworzone przez tego samego producenta zawsze będą dostarczane w tym samym numerze. Podczas wysyłania wiadomości musimy określić klucz; Kafka wygeneruje skrót na podstawie tego klucza i będzie wiedział, która partycja ma dostarczyć tę wiadomość. Ten skrót uwzględnia liczbę partycji tematu; dlatego tego numeru nie można zmienić, gdy temat jest już utworzony.\n\n\nKonsumenci i grupy konsumentów\nKonsumenci to aplikacje zasubskrybowane do jednego lub więcej tematów, które będą odczytywać wiadomości stamtąd. Mogą czytać z jednej lub więcej partycji. Gdy konsument odczytuje tylko z jednej partycji, możemy zapewnić kolejność odczytu, ale gdy pojedynczy konsument odczytuje z dwóch lub więcej partycji, będzie czytać równolegle, więc nie ma gwarancji kolejności odczytu. Na przykład wiadomość, która przyszła później, może zostać odczytana przed inną, która przyszła wcześniej. Dlatego musimy być ostrożni przy wyborze liczby partycji i podczas tworzenia wiadomości.\nInnym ważnym pojęciem Kafki są „Grupy konsumentów”. Jest to bardzo ważne, gdy musimy skalować odczytywanie wiadomości. Staje się to bardzo kosztowne, gdy pojedynczy konsument musi czytać z wielu partycji, więc musimy zrównoważyć obciążenie między naszymi konsumentami, wtedy wchodzą grupy konsumentów.\nDane z jednego tematu będą równoważone obciążeniem między konsumentami, dzięki czemu możemy zagwarantować, że nasi konsumenci będą w stanie obsługiwać i przetwarzać dane. Ideałem jest posiadanie takiej samej liczby konsumentów w grupie, jaką mamy jako partycje w temacie, w ten sposób każdy konsument czyta tylko z jednego. Podczas dodawania konsumentów do grupy należy uważać, jeśli liczba konsumentów jest większa niż liczba partycji, niektórzy konsumenci nie będą czytać z żadnego tematu i pozostaną bezczynni.",
    "crumbs": [
      "222890-D",
      "Wykłady",
      "Architektury strumieniowania danych"
    ]
  },
  {
    "objectID": "indexS.html",
    "href": "indexS.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr letni 2024, SGH Szkoła Główna Handlowa w Warszawie\nSzczegółowy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykładów i ćwiczeń oraz proponowaną literaturę.\nInne książki zamieszczone zostały w zakładce książki"
  },
  {
    "objectID": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "href": "indexS.html#analiza-danych-w-czasie-rzeczywistym",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Kod: 222890-S\nSemestr letni 2024, SGH Szkoła Główna Handlowa w Warszawie\nSzczegółowy opis znajdziesz w sylabusie. Znajdziesz w nim opis wszystkich wykładów i ćwiczeń oraz proponowaną literaturę.\nInne książki zamieszczone zostały w zakładce książki"
  },
  {
    "objectID": "indexS.html#kalendarz",
    "href": "indexS.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\n\n\n02-03-2024 (sobota) 08:00-09:30 - Wykład 1 - G Aula VI\n\nTematy reazlizowane na wykładzie:\n\nDane ustrukturyzowane i nieustrukturyzowane\nProcesy generowania danych\nBig Data\nModele przetwarzania danych OLTP, OLAP\nBiznesowe wymagania dla strumieni danych\n\n\n16-03-2024 (sobota) 08:00-09:30 - Wykład 2 - G Aula VI\n\nTematy:\n\nDefinicje: Zdarzenie, strumień zdarzeń, analiza i przetwarzanie strumieni zdarzeń,\nNarzędzie i aplikacje przetwarzania strumieniowego\nAPI klient-serwer, Pub-Sub i Apache Kafka\n\n\n\n\nlaboratorium\n\n\n06-04-2024 (sobota) 08:00-15:00 - G116 4 grupy\n\n\n07-04-2024 (niedziela) 09:50-17.00 - G116 4 grupy\n\n\n\nWprowadzenie do środowiska Python\nFlask API\n\n\n\n20-04-2024 (sobota) 08:00-15:00 - G116 4 grupy\n\n\n21-04-2024 (niedziela) 09:50-17.00 - G116 4 grupy\n\n\n\ndane ustrukturyzowane\ndane nieustrukturyzowane\nobiektowe podejście do modelowania danych\npodłączenie do bazy relacyjnej (sqlite)\npodłączenie do bazy nierelacyjnej (mongodb)\n\n\n04-05-2024 (sobota) 08:00-15:00 - G116 4 grupy\n05-05-2024 (niedziela) 09:50-17.00 - G116 4 grupy\n25-05-2024 (sobota) 08:00-15:00 - G116 4 grupy\n26-05-2024 (niedziela) 09:50-17.00 - G116 4 grupy\n08-06-2022 (sobota) 08:00-15:00 - G116 4 grupy\n09-06-2022 (niedziela) 09:50-17.00 - G116 4 grupy\n\n\nMiejsce\nWykłady 1-2: G-Aula VI Laboratorium 1-5: 116 G\n\n\nZaliczenie i Egzamin\nWykłady zakończone zostaną testem (ostatnie zajęcia). Pozytywna ocena z testu (powyżej 13 pkt) upoważnia do realizacji ćwiczeń.\nPo ćwiczeniach realizowane będą zadania domowe przekazywane za pośrednictwem platformy teams.\nZaliczenie wszystkich ćwiczeń i zadań upoważnia do realizacji projektu.\nProjekt powinien być realizowany w grupach max 5 osobowych.\nWymagania projektu:\n\nProjekt powinien przedstawiać BIZNESOWY PROBLEM, który można realizować wykorzystując informacje podawane w trybie online. (Nie oznacza to, że nie można korzystać z procesowania batchowego np w celu wygenerowania modelu).\nDane powinny być przesyłane do Apache Kafki i stamtąd poddawane dalszemu procesowaniu i analizie.\nJęzyk programowania jest dowolny - dotyczy każdego komponentu projektu.\nMożna wykorzystać narzędzia BI\nŹródłem danych może być tabela, sztucznie generowane dane, IoT itp."
  },
  {
    "objectID": "indexS.html#technologie",
    "href": "indexS.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nGIT\nPython, Jupyter notebook, Jupyter lab, Colab\nDocker\nApache Spark, Apache Flink, Apache Kafka, Apache Beam\nDatabricks Community edition Web page."
  }
]